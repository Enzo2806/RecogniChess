{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchmetrics \n",
    "from torchvision.models import alexnet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import argparse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import training examples and store them in one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Square Name' 'Image' 'Piece Label']\n",
      " ['A1' array([[[115, 122, 121, 255],\n",
      "               [100, 109, 109, 255],\n",
      "               [ 87,  98,  98, 255],\n",
      "               ...,\n",
      "               [105, 114, 112, 255],\n",
      "               [111, 119, 118, 255],\n",
      "               [118, 125, 124, 255]],\n",
      "\n",
      "              [[100, 109, 108, 255],\n",
      "               [ 79,  92,  91, 255],\n",
      "               [ 61,  78,  76, 255],\n",
      "               ...,\n",
      "               [ 85,  97,  96, 255],\n",
      "               [104, 113, 111, 255],\n",
      "               [121, 127, 126, 255]],\n",
      "\n",
      "              [[ 86,  98,  96, 255],\n",
      "               [ 60,  77,  75, 255],\n",
      "               [ 39,  59,  58, 255],\n",
      "               ...,\n",
      "               [ 68,  83,  81, 255],\n",
      "               [ 94, 105, 103, 255],\n",
      "               [119, 126, 124, 255]],\n",
      "\n",
      "              ...,\n",
      "\n",
      "              [[ 65,  71,  71, 255],\n",
      "               [ 45,  53,  52, 255],\n",
      "               [ 34,  42,  42, 255],\n",
      "               ...,\n",
      "               [ 87,  99,  97, 255],\n",
      "               [109, 117, 116, 255],\n",
      "               [131, 136, 134, 255]],\n",
      "\n",
      "              [[ 82,  87,  87, 255],\n",
      "               [ 66,  73,  72, 255],\n",
      "               [ 58,  65,  64, 255],\n",
      "               ...,\n",
      "               [109, 117, 116, 255],\n",
      "               [125, 131, 130, 255],\n",
      "               [141, 144, 143, 255]],\n",
      "\n",
      "              [[ 99, 103, 102, 255],\n",
      "               [ 88,  92,  92, 255],\n",
      "               [ 82,  87,  86, 255],\n",
      "               ...,\n",
      "               [130, 135, 134, 255],\n",
      "               [140, 143, 142, 255],\n",
      "               [150, 152, 151, 255]]], dtype=uint8) 4]]\n"
     ]
    }
   ],
   "source": [
    "filepath = os.getcwd()+\"/Data Generation/Data Generated/Dataset PreProcessed/EX_0000.npy\"\n",
    "array = np.load(filepath, allow_pickle=True)\n",
    "\n",
    "print(array[0:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The compute_covariance function takes an input data tensor \n",
    "# and returns its covariance matrix. \n",
    "# The formula for the covariance matrix is given by\n",
    "# c = (X^T*X - (1/n)*1*(1^T*X)*(1^T*X)^T)/(n-1), \n",
    "# where X is the input data tensor. \n",
    "# This function first calculates the mean of the columns of the input\n",
    "#  data tensor, and then uses it to calculate the covariance matrix.\n",
    " \n",
    "def compute_covariance(input_data):\n",
    "    \"\"\"\n",
    "    Compute Covariance matrix of the input data\n",
    "    \"\"\"\n",
    "    # first calculates the batch size n of the input data.\n",
    "    n = input_data.size(0)  # batch_size\n",
    "\n",
    "    # Check if using gpu or cpu\n",
    "    # assigns the appropriate device accordingly\n",
    "    if input_data.is_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Create a tensor of size (1, n) with all elements equal to 1, and moves it to the device.\n",
    "    id_row = torch.ones(n).resize(1, n).to(device=device)\n",
    "\n",
    "    # calculate the sum of columns of the input data\n",
    "    # and divide it by n to get the mean of columns mean_column\n",
    "    # (1/n)*(1^T*X)\n",
    "    mean = torch.div(torch.mm(id_row, input_data), n)\n",
    "\n",
    "    # tensor which is the multiplication of the mean of columns with its transpose\n",
    "    # (1/n)*(1^T*X)*(1^T*X)^T\n",
    "    term_mul_2 = torch.mm(mean.t(), mean)\n",
    "\n",
    "    # Calculate the matrix product of the transpose of input data tensor with itself.\n",
    "    # X^T*X\n",
    "    XTX = torch.mm(input_data.t(), input_data)\n",
    "\n",
    "    # Calculate the covariance matrix c\n",
    "    # c = (X^T*X - (1/n)*(1^T*X)*(1^T*X)^T)/(n-1)\n",
    "    c = torch.add(XTX, (-1 * term_mul_2)) * 1 / (n - 1)\n",
    "\n",
    "    # Return the covariance matrix\n",
    "    return c\n",
    "\n",
    "# The coral function takes two input data tensors,\n",
    "#  source and target, and returns the CORAL loss.\n",
    "#  The CORAL loss is given by\n",
    "#  loss = sum((c_s - c_t)^2)/(4*d^2),\n",
    "#  where c_s and c_t are the covariance matrices of the source and target data respectively,\n",
    "#  and d is the dimension of the input data.\n",
    "def coral(source, target):\n",
    "\n",
    "    # first calculates the dimension of the input data d\n",
    "    d = source.size(1)  # dim vector\n",
    "\n",
    "    # Calculate the covariance matrices of the source and target data\n",
    "    source_c = compute_covariance(source)\n",
    "    target_c = compute_covariance(target)\n",
    "\n",
    "    # Calculate the CORAL loss\n",
    "    # loss = sum((c_s - c_t)^2)\n",
    "    loss = torch.sum(torch.mul((source_c - target_c), (source_c - target_c)))\n",
    "\n",
    "    # loss = sum((c_s - c_t)^2)/(4*d^2)\n",
    "    loss = loss / (4 * d * d)\n",
    "    # Return the CORAL loss\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_loader\n",
    "from utils import accuracy, Tracker\n",
    "\n",
    "def train(model, optimizer, source_loader, target_loader, tracker, args, epoch=0):\n",
    "\n",
    "    model.train()\n",
    "    tracker_class, tracker_params = tracker.MovingMeanMonitor, {'momentum': 0.99}\n",
    "\n",
    "    # Trackers to monitor classification and CORAL loss\n",
    "    classification_loss_tracker = tracker.track('classification_loss', tracker_class(**tracker_params))\n",
    "    coral_loss_tracker = tracker.track('CORAL_loss', tracker_class(**tracker_params))\n",
    "\n",
    "    min_n_batches = min(len(source_loader), len(target_loader))\n",
    "\n",
    "    tq = tqdm(range(min_n_batches), desc='{} E{:03d}'.format('Training + Adaptation', epoch), ncols=0)\n",
    "\n",
    "    for _ in tq:\n",
    "\n",
    "        source_data, source_label = next(iter(source_loader))\n",
    "        target_data, _ = next(iter(target_loader))  # Unsupervised Domain Adaptation\n",
    "\n",
    "        source_data, source_label = Variable(source_data.to(device=args.device)), Variable(source_label.to(device=args.device))\n",
    "        target_data = Variable(target_data.to(device=args.device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out_source = model(source_data)\n",
    "        out_target = model(target_data)\n",
    "\n",
    "        classification_loss = F.cross_entropy(out_source, source_label)\n",
    "\n",
    "        # This is where the magic happens\n",
    "        coral_loss = coral(out_source, out_target)\n",
    "        composite_loss = classification_loss + args.lambda_coral * coral_loss\n",
    "\n",
    "        composite_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        classification_loss_tracker.append(classification_loss.item())\n",
    "        coral_loss_tracker.append(coral_loss.item())\n",
    "        fmt = '{:.4f}'.format\n",
    "        tq.set_postfix(classification_loss=fmt(classification_loss_tracker.mean.value),\n",
    "                       coral_loss=fmt(coral_loss_tracker.mean.value))\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, dataset_name, tracker, args, epoch=0):\n",
    "    model.eval()\n",
    "\n",
    "    tracker_class, tracker_params = tracker.MeanMonitor, {}\n",
    "    acc_tracker = tracker.track('{}_accuracy'.format(dataset_name), tracker_class(**tracker_params))\n",
    "\n",
    "    loader = tqdm(data_loader, desc='{} E{:03d}'.format('Evaluating on %s' % dataset_name, epoch), ncols=0)\n",
    "\n",
    "    accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for target_data, target_label in loader:\n",
    "            target_data = Variable(target_data.to(device=args.device))\n",
    "            target_label = Variable(target_label.to(device=args.device))\n",
    "\n",
    "            output = model(target_data)\n",
    "\n",
    "            accuracies.append(accuracy(output, target_label))\n",
    "\n",
    "            acc_tracker.append(sum(accuracies)/len(accuracies))\n",
    "            fmt = '{:.4f}'.format\n",
    "            loader.set_postfix(accuracy=fmt(acc_tracker.mean.value))\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Paper: In the training phase, we set the batch size to 128,\n",
    "    # base learning rate to 10−3, weight decay to 5×10−4, and momentum to 0.9\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Train - Evaluate DeepCORAL model')\n",
    "    parser.add_argument('--disable_cuda', action='store_true',\n",
    "                        help='Disable CUDA')\n",
    "    parser.add_argument('--epochs', type=int, default=50,\n",
    "                        help='Number of total epochs to run')\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='Batch size')\n",
    "    parser.add_argument('--lr', default=1e-3,\n",
    "                        help='Learning Rate')\n",
    "    parser.add_argument('--decay', default=5e-4,\n",
    "                        help='Decay of the learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9,\n",
    "                        help=\"Optimizer's momentum\")\n",
    "    parser.add_argument('--lambda_coral', type=float, default=0.5,\n",
    "                        help=\"Weight that trades off the adaptation with \"\n",
    "                             \"classification accuracy on the source domain\")\n",
    "    parser.add_argument('--source', default='amazon',\n",
    "                        help=\"Source Domain (dataset)\")\n",
    "    parser.add_argument('--target', default='webcam',\n",
    "                        help=\"Target Domain (dataset)\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.device = None\n",
    "\n",
    "    if not args.disable_cuda and torch.cuda.is_available():\n",
    "        args.device = torch.device('cuda')\n",
    "    else:\n",
    "        args.device = torch.device('cpu')\n",
    "\n",
    "    source_train_loader = get_loader(name_dataset=args.source, batch_size=args.batch_size, train=True)\n",
    "    target_train_loader = get_loader(name_dataset=args.target, batch_size=args.batch_size, train=True)\n",
    "\n",
    "    source_evaluate_loader = get_loader(name_dataset=args.source, batch_size=args.batch_size, train=False)\n",
    "    target_evaluate_loader = get_loader(name_dataset=args.target, batch_size=args.batch_size, train=False)\n",
    "\n",
    "    n_classes = len(source_train_loader.dataset.classes)\n",
    "\n",
    "    # ~ Paper : \"We initialized the other layers with the parameters pre-trained on ImageNet\"\n",
    "    # check https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py\n",
    "    model = alexnet(pretrained=True)\n",
    "    # ~ Paper : The dimension of last fully connected layer (fc8) was set to the number of categories (31)\n",
    "    model.classifier[6] = nn.Linear(4096, n_classes)\n",
    "    # ~ Paper : and initialized with N(0, 0.005)\n",
    "    torch.nn.init.normal_(model.classifier[6].weight, mean=0, std=5e-3)\n",
    "\n",
    "    # Initialize bias to small constant number (http://cs231n.github.io/neural-networks-2/#init)\n",
    "    model.classifier[6].bias.data.fill_(0.01)\n",
    "\n",
    "    model = model.to(device=args.device)\n",
    "\n",
    "    # ~ Paper : \"The learning rate of fc8 is set to 10 times the other layers as it was training from scratch.\"\n",
    "    optimizer = torch.optim.SGD([\n",
    "        {'params':  model.features.parameters()},\n",
    "        {'params': model.classifier[:6].parameters()},\n",
    "        # fc8 -> 7th element (index 6) in the Sequential block\n",
    "        {'params': model.classifier[6].parameters(), 'lr': 10 * args.lr}\n",
    "    ], lr=args.lr, momentum=args.momentum)  # if not specified, the default lr is used\n",
    "\n",
    "    tracker = Tracker()\n",
    "\n",
    "    for i in range(args.epochs):\n",
    "        train(model, optimizer, source_train_loader, target_train_loader, tracker, args, i)\n",
    "        evaluate(model, source_evaluate_loader, 'source', tracker, args, i)\n",
    "        evaluate(model, target_evaluate_loader, 'target', tracker, args, i)\n",
    "\n",
    "    # Save logged classification loss, coral loss, source accuracy, target accuracy\n",
    "    torch.save(tracker.to_dict(), \"log.pth\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
