{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b950cda7",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144fa7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now load the dependencies\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# If VSCode doesn't pick up this import, see answer here: \n",
    "# https://stackoverflow.com/questions/65252074/import-path-to-own-script-could-not-be-resolved-pylance-reportmissingimports\n",
    "import sys\n",
    "sys.path.append(\"../../Datasets/\")\n",
    "from Custom_Dataset import * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6211a13b",
   "metadata": {},
   "source": [
    "We can start by setting a seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356ddda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16c12c58190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8c3d84a",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9121b596",
   "metadata": {},
   "source": [
    "We start by defining a custom dataset which loads the data from disk lazily. This is because we have too many training examples to keep all of them in memory at once. The CustomDataset class is defined in Datasets/Cutsom_Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afe02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the generated data\n",
    "train_gen_dataset = CustomDataset(\"Generated\", \"train\", balance = True)\n",
    "val_gen_dataset = CustomDataset(\"Generated\", \"validation\", balance=True)\n",
    "test_gen_dataset = CustomDataset(\"Generated\", \"test\", balance=True)\n",
    "\n",
    "# Extract the real data in full\n",
    "train_real_dataset = CustomDataset(\"Real Life\", \"train\", balance=True)\n",
    "val_real_dataset = CustomDataset(\"Real Life\", \"validation\", balance=True)\n",
    "test_real_dataset = CustomDataset(\"Real Life\", \"test\", balance=True)\n",
    "\n",
    "# Extract a version of the real data where the training set is just 80% of the full validation set\n",
    "# The validation set becomes 20% of the initial validation set\n",
    "train_real_dataset_full_val_subset= CustomDataset(\"Real Life\", \"validation\", balance=True, filter_array = np.arange(0, int(0.8*len(val_real_dataset))))\n",
    "val_real_dataset_full_val_subset = CustomDataset(\"Real Life\", \"validation\", balance=True, filter_array = np.arange(int(0.8*len(val_real_dataset)), len(val_real_dataset)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b93703ce",
   "metadata": {},
   "source": [
    "# Hyperparameter choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b259e4e1",
   "metadata": {},
   "source": [
    "We create a cell to hold the hyperparameters of the model to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ce00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 # Each the real and generated data will be split into batches of this size (Since we only train on generated here)\n",
    "dropout_rate_choices = {0, 0.2, 0.5}\n",
    "gamma_focal_loss_choices = {0, 2, 5} # Choices for the gamma parameter in the focal loss\n",
    "learning_rate = 0.001\n",
    "n_validation = 30 # Number of iterations between each validation run\n",
    "n_validation_minibatches = 5 # Number of minibatches to use for validation every n_validation iterations\n",
    "num_epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e60fc99",
   "metadata": {},
   "source": [
    "# Model implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "868ef8b6",
   "metadata": {},
   "source": [
    "We can start by loading a pre-trained VGG16 model without the classification layers towards the end (Only the feature extractor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a976a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8433f526",
   "metadata": {},
   "source": [
    "We can now visualize its layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9614fd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "018e9568",
   "metadata": {},
   "source": [
    "Because we are looking for a pre-trained feature extractor here, we decide to only use the features part and freeze its weights. We can then add a few subsequent layers to fine tune predictions. We can thus define the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a051bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=13, dropout_rate=0.5):\n",
    "        \n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        # Define the layers of the model\n",
    "        self.features = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4608, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Set the features to not require gradients\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7348424",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3c49179",
   "metadata": {},
   "source": [
    "We can start by finding the device to use for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1237640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.has_mps:\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61cb4352",
   "metadata": {},
   "source": [
    "We can then go ahead and define the loss function we will be using. Because we will opt for a balanced focal loss instead of a regular cross entropy loss which gives more importance to the classes that are harder to classify. We thus implement the focal loss defined by the following formula:\n",
    "$$\n",
    "FL(p_t) = -(1-p_t^{\\gamma})log(p_t)\n",
    "$$\n",
    "\n",
    "where gamma $\\gamma$ is a tunable hyperparameter. We can also further add an alpha term to handle class imbalance, making our loss function a class-balanced focal loss, as shown in https://github.com/AdeelH/pytorch-multi-class-focal-loss. \n",
    "Note: Since we have balanced classes thanks to oversmapling, we will not use the alpha parameter.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "057bb473",
   "metadata": {},
   "source": [
    "Finally, we need an accuracy metric to tune the hyperparameters of the model. We will opt for a balanced accuracy score, which is just regular classification accuracy but adapted to weigh each class by its frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66bd0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the accuracy metrics\n",
    "f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=13, average=\"weighted\").to(DEVICE)\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=13, average=\"weighted\").to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ad6b2b4",
   "metadata": {},
   "source": [
    "We can now load a single example from the loader and display its label as well as its class proportion, which should be around 1/13 which is +- 8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f2ebd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_train_loader = get_loader(train_gen_dataset, batch_size=128)\n",
    "\n",
    "for i, (images, labels) in enumerate(gen_train_loader):\n",
    "    class_proportions = [(labels == i).sum() / len(labels) for i in range(13)]\n",
    "    plt.title(\"label: \" + str(labels[0].item()) + \" proportion: \" + str(class_proportions[labels[0].item()].item() * 100) + \"%\")\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed6cd714",
   "metadata": {},
   "source": [
    "We can also define the following functions for saving the metrics collected during training (Depeding of which training strategy is being used: Training on generated data or on the real life data). We will plot these metrics later in the Results.ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7010d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_json_train_gen(gen_training_losses, gen_training_accs, gen_training_f1s, \\\n",
    "                         gen_validation_losses, gen_validation_accs, gen_validation_f1s, \\\n",
    "                            real_validation_losses, real_validation_accs, real_validation_f1s, \\\n",
    "                                real_validation_accs_full, gamma, batch_size, dropout_rate, learning_rate, \\\n",
    "                                    n_validation, n_validation_minibatches, num_epochs, best_real_acc, best_epoch):\n",
    "        \n",
    "    json_data = {\n",
    "        \"metrics\": {\n",
    "            \"gen_training_losses/iteration\": gen_training_losses,\n",
    "            \"gen_training_accs/iteration\": gen_training_accs,\n",
    "            \"gen_training_f1s/iteration\": gen_training_f1s,\n",
    "            \"gen_validation_losses/n_validation\": gen_validation_losses,\n",
    "            \"gen_validation_accs/n_validation\": gen_validation_accs,\n",
    "            \"gen_validation_f1s/n_validation\": gen_validation_f1s,\n",
    "            \"real_validation_losses/n_validation\": real_validation_losses,\n",
    "            \"real_validation_accs/n_validation\": real_validation_accs,\n",
    "            \"real_validation_f1s/n_validation\": real_validation_f1s,\n",
    "            \"real_validation_accs_full/epoch\": real_validation_accs_full\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"gamma\": gamma,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"n_validation\": n_validation,\n",
    "            \"n_validation_minibatches\": n_validation_minibatches,\n",
    "            \"num_epochs\": num_epochs\n",
    "        },\n",
    "        \"peak_performance\": {\n",
    "            \"best_real_acc\": best_real_acc,\n",
    "            \"best_epoch\": best_epoch\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(f\"TRAIN_GEN/HP tuning results/gamma_{str(gamma)}_batch_{str(batch_size)}_dropout_{str(dropout_rate)}.json\", \"w\") as json_file:\n",
    "        json.dump(json_data, json_file)\n",
    "\n",
    "\n",
    "def save_metrics_to_json_train_real(real_training_losses, real_training_accs, real_training_f1s, \\\n",
    "                                   real_validation_losses, real_validation_accs, real_validation_f1s, \\\n",
    "                                   real_validation_accs_full, gamma, batch_size, dropout_rate, learning_rate, \\\n",
    "                                   n_validation, n_validation_minibatches, num_epochs, best_real_acc, best_epoch, \\\n",
    "                                   full_dataset_used):\n",
    "\n",
    "    json_data = {\n",
    "        \"metrics\": {\n",
    "            \"real_training_losses/iteration\": real_training_losses,\n",
    "            \"real_training_accs/iteration\": real_training_accs,\n",
    "            \"real_training_f1s/iteration\": real_training_f1s,\n",
    "            \"real_validation_losses/n_validation\": real_validation_losses,\n",
    "            \"real_validation_accs/n_validation\": real_validation_accs,\n",
    "            \"real_validation_f1s/n_validation\": real_validation_f1s,\n",
    "            \"real_validation_accs_full/epoch\": real_validation_accs_full\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"gamma\": gamma,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"n_validation\": n_validation,\n",
    "            \"n_validation_minibatches\": n_validation_minibatches,\n",
    "            \"num_epochs\": num_epochs\n",
    "        },\n",
    "        \"peak_performance\": {\n",
    "            \"best_real_acc\": best_real_acc,\n",
    "            \"best_epoch\": best_epoch\n",
    "        }\n",
    "    }\n",
    "\n",
    "    folder = \"TRAIN_FULL_REAL\" if full_dataset_used else \"TRAIN_VAL_REAL\"\n",
    "    with open(f\"{folder}/HP tuning results/gamma_{str(gamma)}_batch_{str(batch_size)}_dropout_{str(dropout_rate)}.json\", \"w\") as json_file:\n",
    "        json.dump(json_data, json_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0e9ca4",
   "metadata": {},
   "source": [
    "We can now define an evaluate function that computes the average accuracy on the full real life validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(basemodel, real_val_loader):\n",
    "\n",
    "    # Set the model to eval mode\n",
    "    basemodel.eval()\n",
    "\n",
    "    # To compute the average validation accuracy\n",
    "    acc_val_sum = 0\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate through the full validation set\n",
    "        for X_val_real, y_val_real in real_val_loader:\n",
    "\n",
    "            # Move the data to the device\n",
    "            X_val_real = X_val_real.to(DEVICE)\n",
    "            y_val_real = y_val_real.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            y_val_pred_prob_real = basemodel(X_val_real)\n",
    "            y_val_pred_real = torch.argmax(y_val_pred_prob_real, dim=1)\n",
    "\n",
    "            # Compute the metrics\n",
    "            acc_val_sum += accuracy(y_val_pred_real, y_val_real)\n",
    "\n",
    "    # Compute the average accuracy\n",
    "    average_real_acc = acc_val_sum / len(real_val_loader)\n",
    "\n",
    "    return average_real_acc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6672cd8",
   "metadata": {},
   "source": [
    "We can now define a training function we will call for each hyperparameter combination. We again need one training function to train on the real life data and an equivalent to train on the generated data. To differentiate between training on the full and on a subset of the validation portion of the real life data in the real life training function, we will add a parameter called `full_dataset_used` to the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2001a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(gamma, dropout_rate, batch_size, hyperparameter_combination_number, \n",
    "                        learning_rate=learning_rate, n_validation=n_validation, \\\n",
    "                        n_validation_minibatches=n_validation_minibatches, num_epochs=num_epochs):\n",
    "    \n",
    "    # Define the data loaders accounting for the batch size\n",
    "    train_loader = get_loader(train_gen_dataset, batch_size=batch_size)\n",
    "    gen_val_loader = get_loader(val_gen_dataset, batch_size=batch_size)\n",
    "    real_val_loader = get_loader(val_real_dataset, batch_size=batch_size)  \n",
    "\n",
    "    # Define the new loss function (Taking into account gamma)\n",
    "    focal_loss = torch.hub.load(\n",
    "        'adeelh/pytorch-multi-class-focal-loss',\n",
    "        model='FocalLoss',\n",
    "        gamma=gamma, # No use of alpha since we have balanced classes now with the oversampling\n",
    "        reduction='mean',\n",
    "        force_reload=False,\n",
    "        verbose = False\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    basemodel = BaseModel(dropout_rate=dropout_rate).to(DEVICE)      \n",
    "    opt = optim.Adam(basemodel.parameters(), lr=learning_rate)\n",
    "\n",
    "    # To store the metrics every iteration on 1 minibatch\n",
    "    gen_training_losses = []\n",
    "    gen_training_accs = []\n",
    "    gen_training_f1s = []\n",
    "\n",
    "    # To store the metrics every n_validation iterations on n_validation_minibatches\n",
    "    gen_validation_losses = []\n",
    "    gen_validation_accs = []\n",
    "    gen_validation_f1s = []\n",
    "    real_validation_losses = []\n",
    "    real_validation_accs = []\n",
    "    real_validation_f1s = []\n",
    "\n",
    "    # To store the accuracy of the epoch on the full real validation set\n",
    "    real_validation_accs_full = []\n",
    "\n",
    "    # To keep track of the best model (Best epoch)\n",
    "    best_real_acc = -1\n",
    "    best_model_state_dict = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Compute the average accuracy on the validation set at epoch 0\n",
    "    average_acc = evaluate_epoch(basemodel, real_val_loader)\n",
    "    real_validation_accs_full.append(average_acc)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train the model\n",
    "        for iteration, (X_train_gen, y_train_gen) in tqdm(enumerate(train_loader)):\n",
    "            \n",
    "            # Set the model to training mode\n",
    "            basemodel.train()\n",
    "\n",
    "            # Move the data to the device\n",
    "            X_train_gen = X_train_gen.to(DEVICE)\n",
    "            y_train_gen = y_train_gen.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            y_train_pred_raw_gen = basemodel(X_train_gen)\n",
    "            y_train_pred_gen = torch.argmax(y_train_pred_raw_gen, dim=1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_train = focal_loss(y_train_pred_raw_gen, y_train_gen.long())\n",
    "\n",
    "            # Compute the accuracy\n",
    "            acc_train = accuracy(y_train_pred_gen, y_train_gen)\n",
    "            f1_train = f1_score(y_train_pred_gen, y_train_gen)\n",
    "\n",
    "            # Backward pass\n",
    "            opt.zero_grad()\n",
    "            loss_train.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Store the loss & accuracy\n",
    "            gen_training_losses.append(loss_train.item())\n",
    "            gen_training_accs.append(acc_train.item())\n",
    "            gen_training_f1s.append(f1_train.item())\n",
    "            \n",
    "            # Check if the model should be validated\n",
    "            if iteration == 0 or (iteration + 1) % n_validation == 0:\n",
    "                \n",
    "                # Set the model to evaluation mode\n",
    "                basemodel.eval()\n",
    "                \n",
    "                # Disable gradient calculation\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    # 1) Evaluate on the generated validation set\n",
    "                    acc_val_sum = 0\n",
    "                    weighted_f1_val_sum = 0\n",
    "                    loss_val_sum = 0\n",
    "\n",
    "                    # Extract an iterator from the generated data loader\n",
    "                    gen_val_iter = iter(gen_val_loader)\n",
    "\n",
    "                    # Iterate for n_validation_minibatches\n",
    "                    for _ in range(n_validation_minibatches):\n",
    "\n",
    "                        # Get the next minibatch\n",
    "                        minibatch = next(gen_val_iter, None)\n",
    "                        if minibatch is None:\n",
    "                            gen_val_iter = iter(gen_val_loader)\n",
    "                            minibatch = next(gen_val_iter)\n",
    "                            \n",
    "                        # Extract the data\n",
    "                        X_val_gen, y_val_gen = minibatch\n",
    "\n",
    "                        # Move the data to the device\n",
    "                        X_val_gen = X_val_gen.to(DEVICE)\n",
    "                        y_val_gen = y_val_gen.to(DEVICE)\n",
    "\n",
    "                        # Forward pass\n",
    "                        y_val_pred_raw_gen = basemodel(X_val_gen)\n",
    "                        y_val_pred_gen = torch.argmax(y_val_pred_raw_gen, dim=1)\n",
    "\n",
    "                        # Compute the metrics\n",
    "                        acc_val_sum += accuracy(y_val_pred_gen, y_val_gen)\n",
    "                        weighted_f1_val_sum += f1_score(y_val_pred_gen, y_val_gen)\n",
    "                        loss_val_sum += focal_loss(y_val_pred_raw_gen, y_val_gen)\n",
    "\n",
    "                    # Compute the average metrics\n",
    "                    acc_val_gen = acc_val_sum / n_validation_minibatches\n",
    "                    loss_val_gen = loss_val_sum / n_validation_minibatches\n",
    "                    weighted_f1_val_gen = weighted_f1_val_sum / n_validation_minibatches\n",
    "\n",
    "\n",
    "                    # 2) Repeat on the real validation set\n",
    "\n",
    "                    acc_val_sum = 0\n",
    "                    weighted_f1_val_sum = 0\n",
    "                    loss_val_sum = 0\n",
    "\n",
    "                    # Extract an iterator from the generated data loader\n",
    "                    real_val_iter = iter(real_val_loader)\n",
    "\n",
    "                    # Iterate for n_validation_minibatches\n",
    "                    for _ in range(n_validation_minibatches):\n",
    "\n",
    "                        # Get the next minibatch\n",
    "                        minibatch = next(real_val_iter, None)\n",
    "                        if minibatch is None:\n",
    "                            real_val_iter = iter(real_val_loader)\n",
    "                            minibatch = next(real_val_iter)\n",
    "\n",
    "                        # Extract the data\n",
    "                        X_val_real, y_val_real = minibatch\n",
    "                            \n",
    "                        # Move the data to the device\n",
    "                        X_val_real = X_val_real.to(DEVICE)\n",
    "                        y_val_real = y_val_real.to(DEVICE)\n",
    "\n",
    "                        # Forward pass\n",
    "                        y_val_pred_raw_real = basemodel(X_val_real)\n",
    "                        y_val_pred_real = torch.argmax(y_val_pred_raw_real, dim=1)\n",
    "\n",
    "                        # Compute the metrics\n",
    "                        acc_val_sum += accuracy(y_val_pred_real, y_val_real)\n",
    "                        weighted_f1_val_sum += f1_score(y_val_pred_real, y_val_real)\n",
    "                        loss_val_sum += focal_loss(y_val_pred_raw_real, y_val_real)\n",
    "\n",
    "                    # Compute the average metrics\n",
    "                    acc_val_real = acc_val_sum / n_validation_minibatches\n",
    "                    loss_val_real = loss_val_sum / n_validation_minibatches\n",
    "                    weighted_f1_val_real = weighted_f1_val_sum / n_validation_minibatches\n",
    "\n",
    "                    # Store all 6 metrics\n",
    "                    gen_validation_losses.append(loss_val_gen.item())\n",
    "                    gen_validation_accs.append(acc_val_gen.item())\n",
    "                    gen_validation_f1s.append(weighted_f1_val_gen.item())\n",
    "                    real_validation_losses.append(loss_val_real.item())\n",
    "                    real_validation_accs.append(acc_val_real.item())\n",
    "                    real_validation_f1s.append(weighted_f1_val_real.item())\n",
    "\n",
    "                    # Print an update -- Only show the last print statement\n",
    "                    clear_output(wait=True)\n",
    "                    print('----------------------------------------------------------------')\n",
    "                    print(f'TRAINING HP COMBINATION  [#{hyperparameter_combination_number}] -- EPOCH [{epoch+1}] --  ITERATION [{iteration+1}]')\n",
    "                    print(f'CURRENT BEST EPOCH: {best_epoch} -- CURRENT BEST FULL VALIDATION SET ACCURACY: {best_real_acc}')\n",
    "                    print('----------------------------------------------------------------')\n",
    "                    print(f'TRAINING => Loss: {loss_train} -- Acc: {acc_train} -- F1: {f1_train}')\n",
    "                    print(f'GENERATED VALIDATION => Loss: {loss_val_gen} -- Acc: {acc_val_gen} -- F1: {weighted_f1_val_gen}')\n",
    "                    print(f'REAL VALIDATION => Loss: {loss_val_real} -- Acc: {acc_val_real} -- F1: {weighted_f1_val_real}')\n",
    "                    print('----------------------------------------------------------------')\n",
    "                    \n",
    "        # Save the model every epoch as a checkpoint \n",
    "        torch.save(basemodel.state_dict(), f'./TRAIN_GEN/checkpoints/basemodel_gamma_{gamma}_dropout_{dropout_rate}_batch_{batch_size}_epoch_{epoch+1}.ckpt')\n",
    "\n",
    "        # Check whether to overwrite the best model by computing the validation accuracy on the full real life validation set\n",
    "        average_real_acc = evaluate_epoch(basemodel, real_val_loader)\n",
    "\n",
    "        # Append the average real validation accuracy of the epoch to the corresponding array\n",
    "        real_validation_accs_full.append(average_real_acc.item())\n",
    "\n",
    "        # Check whether the current version of the model is the best one\n",
    "        if best_model_state_dict is None or average_real_acc > best_real_acc:\n",
    "            best_real_acc = average_real_acc.item()\n",
    "            best_model_state_dict = basemodel.state_dict()\n",
    "            best_epoch = epoch + 1\n",
    "        \n",
    "    # Plot and save the metrics\n",
    "    save_metrics_to_json_train_gen(gen_training_losses, gen_training_accs, gen_training_f1s, gen_validation_losses, \\\n",
    "                                gen_validation_accs, gen_validation_f1s, real_validation_losses, real_validation_accs, \\\n",
    "                                real_validation_f1s, real_validation_accs_full, gamma, batch_size, dropout_rate, learning_rate, \\\n",
    "                                n_validation, n_validation_minibatches, num_epochs, best_real_acc, best_epoch)\n",
    "    \n",
    "    return best_model_state_dict, best_real_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e886d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_real(full_dataset_used, gamma, dropout_rate, batch_size, hyperparameter_combination_number,\n",
    "              learning_rate=learning_rate, n_validation=n_validation,\n",
    "              n_validation_minibatches=n_validation_minibatches, num_epochs=num_epochs):\n",
    "\n",
    "    # Define the data loaders accounting for the batch size\n",
    "    if full_dataset_used:\n",
    "        train_loader = get_loader(train_real_dataset, batch_size=batch_size)\n",
    "        val_loader = get_loader(val_real_dataset, batch_size=batch_size)\n",
    "    else:\n",
    "        train_loader = get_loader(train_real_dataset_full_val_subset, batch_size=batch_size)\n",
    "        val_loader = get_loader(val_real_dataset_full_val_subset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    # Define the new loss function (Taking into account gamma)\n",
    "    focal_loss = torch.hub.load(\n",
    "        'adeelh/pytorch-multi-class-focal-loss',\n",
    "        model='FocalLoss',\n",
    "        gamma=gamma,  # No use of alpha since we have balanced classes now with the oversampling\n",
    "        reduction='mean',\n",
    "        force_reload=False,\n",
    "        verbose=False\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    basemodel = BaseModel(dropout_rate=dropout_rate).to(DEVICE)\n",
    "    opt = optim.Adam(basemodel.parameters(), lr=learning_rate)\n",
    "\n",
    "    # To store the metrics every iteration on 1 minibatch\n",
    "    training_losses = []\n",
    "    training_accs = []\n",
    "    training_f1s = []\n",
    "\n",
    "    # To store the metrics every n_validation iterations on n_validation_minibatches\n",
    "    validation_losses = []\n",
    "    validation_accs = []\n",
    "    validation_f1s = []\n",
    "\n",
    "    # To store the accuracy of the epoch on the full real validation set\n",
    "    validation_accs_full = []\n",
    "\n",
    "    # Folder to save metrics to\n",
    "    folder = \"TRAIN_FULL_REAL\" if full_dataset_used else \"TRAIN_VAL_REAL\"\n",
    "\n",
    "    # To keep track of the best model (Best epoch)\n",
    "    best_acc = -1\n",
    "    best_model_state_dict = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Compute the average accuracy on the validation set at epoch 0\n",
    "    average_acc = evaluate_epoch(basemodel, val_loader)\n",
    "    validation_accs_full.append(average_acc)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Train the model\n",
    "        for iteration, (X_train, y_train) in tqdm(enumerate(train_loader)):\n",
    "\n",
    "            # Set the model to training mode\n",
    "            basemodel.train()\n",
    "\n",
    "            # Move the data to the device\n",
    "            X_train = X_train.to(DEVICE)\n",
    "            y_train = y_train.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            y_train_pred_raw = basemodel(X_train)\n",
    "            y_train_pred = torch.argmax(y_train_pred_raw, dim=1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_train = focal_loss(y_train_pred_raw, y_train.long())\n",
    "\n",
    "            # Compute the accuracy\n",
    "            acc_train = accuracy(y_train_pred, y_train)\n",
    "            f1_train = f1_score(y_train_pred, y_train)\n",
    "\n",
    "            # Backward pass\n",
    "            opt.zero_grad()\n",
    "            loss_train.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Store the loss & accuracy\n",
    "            training_losses.append(loss_train.item())\n",
    "            training_accs.append(acc_train.item())\n",
    "            training_f1s.append(f1_train.item())\n",
    "\n",
    "            # Check if the model should be validated\n",
    "            if iteration == 0 or (iteration + 1) % n_validation == 0:\n",
    "\n",
    "                # Set the model to evaluation mode\n",
    "                basemodel.eval()\n",
    "\n",
    "                # Disable gradient calculation\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    # Compute all metrics on the real life validation dataset\n",
    "                    acc_val_sum = 0\n",
    "                    weighted_f1_val_sum = 0\n",
    "                    loss_val_sum = 0\n",
    "\n",
    "                    # Extract an iterator from the generated data loader\n",
    "                    val_iter = iter(val_loader)\n",
    "\n",
    "                    # Iterate for n_validation_minibatches\n",
    "                    for _ in range(n_validation_minibatches):\n",
    "\n",
    "                        # Get the next minibatch\n",
    "                        minibatch = next(val_iter, None)\n",
    "                        if minibatch is None:\n",
    "                            val_iter = iter(val_loader)\n",
    "                            minibatch = next(val_iter)\n",
    "\n",
    "                        # Extract the data\n",
    "                        X_val, y_val = minibatch\n",
    "\n",
    "                        # Move the data to the device\n",
    "                        X_val = X_val.to(DEVICE)\n",
    "                        y_val = y_val.to(DEVICE)\n",
    "\n",
    "                        # Forward pass\n",
    "                        y_val_pred_raw = basemodel(X_val)\n",
    "                        y_val_pred = torch.argmax(\n",
    "                            y_val_pred_raw, dim=1)\n",
    "\n",
    "                        # Compute the metrics\n",
    "                        acc_val_sum += accuracy(y_val_pred, y_val)\n",
    "                        weighted_f1_val_sum += f1_score(y_val_pred, y_val)\n",
    "                        loss_val_sum += focal_loss(y_val_pred_raw, y_val)\n",
    "\n",
    "                    # Compute the average metrics\n",
    "                    acc_val = acc_val_sum / n_validation_minibatches\n",
    "                    loss_val = loss_val_sum / n_validation_minibatches\n",
    "                    weighted_f1_val = weighted_f1_val_sum / n_validation_minibatches\n",
    "\n",
    "                    # Store all 6 metrics\n",
    "                    validation_losses.append(loss_val.item())\n",
    "                    validation_accs.append(acc_val.item())\n",
    "                    validation_f1s.append(weighted_f1_val.item())\n",
    "\n",
    "                    # Print an update -- Only show the last print statement\n",
    "                    clear_output(wait=True)\n",
    "                    print('----------------------------------------------------------------')\n",
    "                    print(f'TRAINING HP COMBINATION  [#{hyperparameter_combination_number}] -- EPOCH [{epoch+1}] --  ITERATION [{iteration+1}]')\n",
    "                    print(f'CURRENT BEST EPOCH: {best_epoch} -- CURRENT BEST FULL VALIDATION SET ACCURACY: {best_acc}')\n",
    "                    print('----------------------------------------------------------------')\n",
    "                    print(f'TRAINING => Loss: {loss_train} -- Acc: {acc_train} -- F1: {f1_train}')\n",
    "                    print(f'VALIDATION => Loss: {loss_val} -- Acc: {acc_val} -- F1: {weighted_f1_val}')\n",
    "                    print('----------------------------------------------------------------')\n",
    "\n",
    "        # Save the model every epoch as a checkpoint\n",
    "        torch.save(basemodel.state_dict(), f'./{folder}/checkpoints/basemodel_gamma_{gamma}_dropout_{dropout_rate}_epoch_{epoch+1}.ckpt')\n",
    "\n",
    "        # Check whether to overwrite the best model by computing the validation accuracy on the full real life validation set\n",
    "\n",
    "        # Compute the average acc on the full real validation set\n",
    "        average_acc = evaluate_epoch(basemodel, val_loader)\n",
    "\n",
    "        # Append the average real validation accuracy of the epoch to the corresponding array\n",
    "        validation_accs_full.append(average_acc.item())\n",
    "\n",
    "        # Check whether the current version of the model is the best one\n",
    "        if best_model_state_dict is None or average_acc > best_acc:\n",
    "            best_acc = average_acc.item()\n",
    "            best_model_state_dict = basemodel.state_dict()\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "    # Plot and save the metrics\n",
    "    save_metrics_to_json_train_real(training_losses, training_accs, training_f1s, validation_losses,\n",
    "                                   validation_accs, validation_f1s, validation_accs_full, gamma, batch_size, dropout_rate, learning_rate,\n",
    "                                   n_validation, n_validation_minibatches, num_epochs, best_acc, best_epoch, full_dataset_used)\n",
    "\n",
    "    return best_model_state_dict, best_acc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e0fb873",
   "metadata": {},
   "source": [
    "We can now proceed to train our model on the generated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3473931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r1240it [14:48,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "TRAINING HP COMBINATION  [#6] -- EPOCH [4] --  ITERATION [1240]\n",
      "CURRENT BEST EPOCH: 1 -- CURRENT BEST FULL VALIDATION SET ACCURACY: 0.6075000166893005\n",
      "----------------------------------------------------------------\n",
      "TRAINING => Loss: 0.0010532764717936516 -- Acc: 1.0 -- F1: 1.0\n",
      "GENERATED VALIDATION => Loss: 0.001165767665952444 -- Acc: 0.9979999661445618 -- F1: 0.9979999661445618\n",
      "REAL VALIDATION => Loss: 4.9985198974609375 -- Acc accuracy: 0.518000066280365 -- F1: 0.49486303329467773\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1242it [14:48,  1.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\Models\\BASE\\Simple model.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m dropout_rate \u001b[39min\u001b[39;00m dropout_rate_choices:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_size \u001b[39min\u001b[39;00m batch_size_choices:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         cur_best_model_state_dict, cur_best_real_acc \u001b[39m=\u001b[39m train_gen(gamma, dropout_rate, batch_size, model_count)  \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m# Create a row to add to the dataframe\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         row \u001b[39m=\u001b[39m [gamma, batch_size, dropout_rate, cur_best_real_acc]\n",
      "\u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\Models\\BASE\\Simple model.ipynb Cell 32\u001b[0m in \u001b[0;36mtrain_gen\u001b[1;34m(gamma, dropout_rate, batch_size, hyperparameter_combination_number, learning_rate, n_validation, n_validation_minibatches, num_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m best_epoch \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mfor\u001b[39;00m iteration, (X_train_gen, y_train_gen) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_loader)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         \u001b[39m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         basemodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Models/BASE/Simple%20model.ipynb#X43sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         \u001b[39m# Move the data to the device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\Models\\BASE\\../../Datasets\\Custom_Dataset.py:95\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     92\u001b[0m img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_path, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages[idx])\n\u001b[0;32m     94\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\n\u001b[1;32m---> 95\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[0;32m     97\u001b[0m \u001b[39m# Convert the label to int instead of float\u001b[39;00m\n\u001b[0;32m     98\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx]\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:719\u001b[0m, in \u001b[0;36mRandomHorizontalFlip.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    712\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[39m        PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 719\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39;49mrand(\u001b[39m1\u001b[39;49m) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp:\n\u001b[0;32m    720\u001b[0m         \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mhflip(img)\n\u001b[0;32m    721\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# To store the best model\n",
    "best_real_acc = -1\n",
    "\n",
    "# Store the validation accuracies\n",
    "hp_final_accs = pd.DataFrame(columns=[\"Gamma\", \"Dropout Rate\", \"Best Validation Accuracy\"])\n",
    "\n",
    "# To keep track of the number of models trained\n",
    "model_count = 1\n",
    "\n",
    "for gamma in gamma_focal_loss_choices:\n",
    "\n",
    "    for dropout_rate in dropout_rate_choices:\n",
    "        \n",
    "        # Train the model\n",
    "        cur_best_model_state_dict, cur_best_real_acc = train_gen(gamma, dropout_rate, batch_size, model_count)  \n",
    "\n",
    "        # Create a row to add to the dataframe\n",
    "        row = [gamma, dropout_rate, cur_best_real_acc]\n",
    "        \n",
    "        # Store it\n",
    "        hp_final_accs.loc[len(hp_final_accs)] = row\n",
    "\n",
    "        # Compare to the best model\n",
    "        if cur_best_real_acc > best_real_acc:\n",
    "            best_real_acc = cur_best_real_acc\n",
    "            torch.save(cur_best_model_state_dict, f'./TRAIN_GEN/best_BASE_TRAIN_GEN_model.ckpt')\n",
    "\n",
    "        # Save (overwrite) the dataframe as a table every time a model finish training so that we can keep track of the progress\n",
    "        hp_final_accs.to_csv('./TRAIN_GEN/HP_best_real_accuracy_comparison_table.csv', index=False)\n",
    "\n",
    "        # Increment the model count\n",
    "        model_count += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9145aade",
   "metadata": {},
   "source": [
    "Next, we train based on an 80% split of the validation subset of the real-life data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2925738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the best model\n",
    "best_real_acc = -1\n",
    "\n",
    "# Store the validation accuracies\n",
    "hp_final_accs = pd.DataFrame(columns=[\"Gamma\", \"Dropout Rate\", \"Best Validation Accuracy\"])\n",
    "\n",
    "# To keep track of the number of models trained\n",
    "model_count = 1\n",
    "\n",
    "for gamma in gamma_focal_loss_choices:\n",
    "\n",
    "    for dropout_rate in dropout_rate_choices:\n",
    "\n",
    "        # Train the model\n",
    "        cur_best_model_state_dict, cur_best_real_acc = train_real(False, gamma, dropout_rate, batch_size, model_count)\n",
    "\n",
    "        # Create a row to add to the dataframe\n",
    "        row = [gamma, dropout_rate, cur_best_real_acc]\n",
    "\n",
    "        # Store it\n",
    "        hp_final_accs.loc[len(hp_final_accs)] = row\n",
    "\n",
    "        # Compare to the best model\n",
    "        if cur_best_real_acc > best_real_acc:\n",
    "            best_real_acc = cur_best_real_acc\n",
    "            torch.save(cur_best_model_state_dict,\n",
    "                        f'./TRAIN_VAL_REAL/best_BASE_TRAIN_VAL_REAL_model.ckpt')\n",
    "\n",
    "        # Save (overwrite) the dataframe as a table every time a model finish training so that we can keep track of the progress\n",
    "        hp_final_accs.to_csv(\n",
    "            './TRAIN_VAL_REAL/HP_best_real_accuracy_comparison_table.csv', index=False)\n",
    "        \n",
    "        # Increment the model count\n",
    "        model_count += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64e5fcf3",
   "metadata": {},
   "source": [
    "Finally, we use the full training real-life data as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867043f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the best model\n",
    "best_real_acc = -1\n",
    "\n",
    "# Store the validation accuracies\n",
    "hp_final_accs = pd.DataFrame(\n",
    "    columns=[\"Gamma\", \"Dropout Rate\", \"Best Validation Accuracy\"])\n",
    "\n",
    "# To keep track of the number of models trained\n",
    "model_count = 1\n",
    "\n",
    "for gamma in gamma_focal_loss_choices:\n",
    "\n",
    "    for dropout_rate in dropout_rate_choices:\n",
    "\n",
    "        # Train the model\n",
    "        cur_best_model_state_dict, cur_best_real_acc = train_real(True, gamma, dropout_rate, batch_size, model_count)\n",
    "\n",
    "        # Create a row to add to the dataframe\n",
    "        row = [gamma, dropout_rate, cur_best_real_acc]\n",
    "\n",
    "        # Store it\n",
    "        hp_final_accs.loc[len(hp_final_accs)] = row\n",
    "\n",
    "        # Compare to the best model\n",
    "        if cur_best_real_acc > best_real_acc:\n",
    "            best_real_acc = cur_best_real_acc\n",
    "            torch.save(cur_best_model_state_dict,\n",
    "                        f'./TRAIN_FULL_REAL/best_BASE_TRAIN_FULL_REAL_model.ckpt')\n",
    "\n",
    "        # Save (overwrite) the dataframe as a table every time a model finish training so that we can keep track of the progress\n",
    "        hp_final_accs.to_csv(\n",
    "            './TRAIN_FULL_REAL/HP_best_real_accuracy_comparison_table.csv', index=False)\n",
    "        \n",
    "        # Increment the model count\n",
    "        model_count += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
