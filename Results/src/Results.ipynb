{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is made to compare and evaluate each \"best model\" we obtained for DANN, CORAL and BASE model (VGG-16 trained on the generated data). All models are trained and tuned in their corresponding files (in the Models folder). This file evaluates them, and compares the best models obtained after hyperparameter tuning based on the result of multiple metrics: testing accuracy, f1-score, confusion matrix and average AUPRC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "import torchmetrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "# If VSCode doesn't pick up this import, see answer here: \n",
    "# https://stackoverflow.com/questions/65252074/import-path-to-own-script-could-not-be-resolved-pylance-reportmissingimports\n",
    "import sys\n",
    "sys.path.append(\"../../Datasets/\")\n",
    "from Custom_Dataset import * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the device to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.has_mps:\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training and Validation Curves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot all the metrics collected during the training of the models. These metrics were stored in the corresponding JSON files. We will only plot the metrics for the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JSON files containing the results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the classes for each model. The base model and CORAL model have one class, while the DANN has two classes: one for the feature extractor and one for the classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=4608, num_classes=13):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        c = self.layer(h)\n",
    "        return c\n",
    "    \n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "        Feature Extractor\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        # Import the VGG16 model\n",
    "        self.conv = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "\n",
    "        # Freeze all the weights in modules 0 up-to and including 25\n",
    "        for param in self.conv[:25].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoralModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=13):\n",
    "        \n",
    "        super(CoralModel, self).__init__()\n",
    "        \n",
    "        # Define the layers of the model\n",
    "        self.features = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4608, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Freeze all the weights in modules 0 up-to and including 25\n",
    "        for param in self.features[:25].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.features(x)\n",
    "        h = torch.flatten(h, 1)\n",
    "        output = self.classifier(h)\n",
    "        return h, output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class for the Base Model\n",
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=13, dropout_rate=0.5):\n",
    "        \n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        # Define the layers of the model\n",
    "        self.features = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4608, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Set the features to not require gradients\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the models we saved as the best ones for each model type (DANN, CORAL, and Base Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DANN_path = \"../../Models/DANN/\"\n",
    "DANN_C_path = DANN_path + \"best_model_C.ckpt\"\n",
    "DANN_F_path = DANN_path + \"best_model_F.ckpt\"\n",
    "CORAL_path = \"../../Models/CORAL/best_CORAL_model.ckpt\"\n",
    "BASE_path = \"../../Models/BASE/TRAIN_GEN/best_BASE_TRAIN_GEN_model.ckpt\"\n",
    "\n",
    "# # Instantiate the classifier model object for the dann model\n",
    "# DANN_C = Classifier().to(DEVICE)\n",
    "# # Load the state dictionary from the checkpoint file\n",
    "# state_dict = torch.load(DANN_C_path, map_location=torch.device(DEVICE))\n",
    "# # Load the state dictionary into the model\n",
    "# DANN_C.load_state_dict(state_dict)\n",
    "# # Put the model into evaluation mode\n",
    "# DANN_C.eval()\n",
    "\n",
    "# # Instantiate the feature extractor model object for the dann model\n",
    "# DANN_F = FeatureExtractor().to(DEVICE)\n",
    "# # Load the state dictionary from the checkpoint file\n",
    "# state_dict = torch.load(DANN_F_path, map_location=torch.device(DEVICE))\n",
    "# # Load the state dictionary into the model\n",
    "# DANN_F.load_state_dict(state_dict)\n",
    "# # Put the model into evaluation mode\n",
    "# DANN_F.eval()\n",
    "\n",
    "# Instantiate the model object for the coral model\n",
    "coral_model = CoralModel().to(DEVICE)\n",
    "# Load the state dictionary from the checkpoint file\n",
    "state_dict = torch.load(CORAL_path, map_location=torch.device(DEVICE))\n",
    "# Load the state dictionary into the model\n",
    "coral_model.load_state_dict(state_dict)\n",
    "# Put the model into evaluation mode\n",
    "coral_model.eval()\n",
    "\n",
    "# Instantiate the model object for the base model\n",
    "base_model = BaseModel().to(DEVICE)\n",
    "# Load the state dictionary from the checkpoint file\n",
    "state_dict = torch.load(BASE_path, map_location=torch.device(DEVICE))\n",
    "# Load the state dictionary into the model\n",
    "base_model.load_state_dict(state_dict)\n",
    "# Put the model into evaluation mode\n",
    "base_model.eval()\n",
    "\n",
    "# To not show the Base architecture\n",
    "print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and metric collection functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the performance of the best models using the testing accuracy, f1-score, confusion matrix and average AUPRC metrics. \n",
    "\n",
    "* Specifically, for the testing accuracy, we will use the balanced accuracy score, which is similar to regular classification accuracy, but it takes into account the frequency of each class. The balanced accuracy score will be computed using on both the real-life dataset and the generated dataset, however, our ultimate goal is to predict labels for real-life images.\n",
    "\n",
    "* As a reminder, the f1-score is a performance metric that provides a balance between the precision and recall of the model. The f1-score ranges from 0 to 1, with a higher score indicating a better performance of the model. The equation for f1-score is: $$ f1 = \\frac{2 (precision * recall)}{precision + recall}$$ where $precision$ is the number of true positives (correctly predicted positive samples) divided by the total number of predicted positive samples, and $recall$ is the number of true positives divided by the total number of actual positive samples. In other words, f1-score takes into account both the model's ability to correctly identify positive samples (recall) and its tendency to not mislabel negative samples as positive (precision). \n",
    "\n",
    "* The confusion matrix compares the predicted class labels with the true class labels and counts the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) for each class. It can help identify which classes the model is having difficulty classifying correctly, and whihc classes it can predict with ease.\n",
    "\n",
    "* Finally, the AUPRC measures the overall quality of the model's predictions, taking into account both precision and recall across all possible classification thresholds. A high average AUPRC indicates that the model has a good balance of precision and recall across all possible classification thresholds, and is able to accurately distinguish between positive and negative cases. Conversely, a low average AUPRC indicates poor performance, and suggests that the model is making many incorrect predictions or missing many true positive cases.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the balanced accuracy function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the balanced accuracy function\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=13, average=\"weighted\").to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the final testing accuracies of each model (DANN, CORAL, and Base model), we evaluate their performance on both the real-life dataset and the generated dataset. Additionally, we analyze the metrics with and without oversampling. For this purpose, we use a function that takes the model(s), batch size, dataset and oversampling boolean as input to compute the testing accuracy, f1-score, confusion matrix and average AUPRC. \n",
    "\n",
    "It's important to note that the most important metrics we will use for comparison are the one on the real-life dataset without oversampling (so a classic distribution of chess boards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing accuracy evaluation function\n",
    "def evaluate(model, batch_size, dataset, model2=None, oversampling=True, coral=False):\n",
    "    # We use our custom dataset and loader defined in the \"Datasets\" folder.\n",
    "    test_dataset = CustomDataset(dataset, \"test\", balance=oversampling)\n",
    "    loader = get_loader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Makes sur emodel is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Set accumulated accuracy to 0\n",
    "    acc = 0\n",
    "\n",
    "    y_true = [] # Ground truth labels\n",
    "    y_hat = [] # Predicted labels\n",
    "\n",
    "    # Remove grad\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            # Move the data to the device\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            if coral:\n",
    "                # Forward pass\n",
    "                _, y_pred_prob = model(images)\n",
    "            else:\n",
    "                # Forward pass\n",
    "                y_pred_prob = model(images)\n",
    "\n",
    "            # If there is a second model \n",
    "            # (for DANN we first gothrough the Feature extractor and then the Classifier)\n",
    "            if model2:\n",
    "                y_pred_prob = model2(y_pred_prob)\n",
    "\n",
    "            y_pred = torch.argmax(y_pred_prob, dim=1)\n",
    "\n",
    "            # Compute the metrics\n",
    "            acc +=  accuracy(y_pred, labels)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_hat.extend(y_pred.cpu().numpy())\n",
    "\n",
    "        # Compute the average accuracy\n",
    "        final_accuracy = acc / len(loader)\n",
    "        \n",
    "        # Compute the f1 score\n",
    "        f1 = f1_score(y_true, y_hat, average='weighted')\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_hat, normalize='true')\n",
    "\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_hat = np.asarray(y_hat)\n",
    "\n",
    "        n_classes = 13\n",
    "        from sklearn.preprocessing import LabelBinarizer\n",
    "        lb = LabelBinarizer()\n",
    "        y_true_binary = lb.fit_transform(y_true)\n",
    "        y_hat_binary = lb.fit_transform(y_hat)\n",
    "\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        for i in range(n_classes):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_true_binary[:, i], y_hat_binary[:, i])\n",
    "            \n",
    "        auprc = dict()\n",
    "        for i in range(n_classes):\n",
    "            auprc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "        average_auprc = np.mean(list(auprc.values()))\n",
    "\n",
    "    # Return all metrics computed\n",
    "    return final_accuracy, f1, cm, average_auprc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a last function \"get_metrics\" to obtain all the metrics we defined above for all models depending on the dataset and if the data is oversampled or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the batch size to 32 for all accuracy computations.\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to get the testing accuracy, f1 score, confusion matrix and average AUPRC for all DANN, CORAL and Base models.\n",
    "It takes as input:\n",
    "- the dataset to evaluate the metrics on (\"Real Life\" or \"Generated\")\n",
    "- if oversampling must be performed\n",
    "'''\n",
    "def get_metrics(dataset, oversampling):\n",
    "    # Create empty dictionaries to store the results in\n",
    "    # We will save these dictionaries in JSON files\n",
    "    accuracies = {}\n",
    "    f1s = {}\n",
    "    auprcs = {}\n",
    "\n",
    "    # We will not save the confusion matrix as JSON, but we will save it as a numpy array\n",
    "    # We will direclty save it using matplotlib\n",
    "    confusion_matrices = np.empty([0]) \n",
    "    \n",
    "    # # Evaluate the DANN model\n",
    "    # accuracy, f1, conf, auprc= evaluate(DANN_F, batch_size, dataset, DANN_C, oversampling=oversampling)\n",
    "    # # Append results\n",
    "    # accuracies[\"DANN\"] = accuracy.item()\n",
    "    # f1s[\"DANN\"] = f1.item()\n",
    "    # auprcs[\"DANN\"] = auprc\n",
    "    # confusion_matrices = np.append(confusion_matrices, conf)\n",
    "\n",
    "    # # Evaluate the CORAL model\n",
    "    # accuracy, f1, conf, auprc= evaluate(coral_model, batch_size, dataset, oversampling=oversampling, coral=True)\n",
    "    # # Append results\n",
    "    # accuracies[\"CORAL\"] = accuracy.item()\n",
    "    # f1s[\"CORAL\"] = f1.item()\n",
    "    # auprcs[\"CORAL\"] = auprc\n",
    "    # confusion_matrices = np.append(confusion_matrices, conf)\n",
    "\n",
    "    # Evaluate the Base model\n",
    "    accuracy, f1, conf, auprc = evaluate(base_model, batch_size, dataset, oversampling=oversampling)\n",
    "    # Append results\n",
    "    accuracies[\"BASE\"] = accuracy.item()\n",
    "    f1s[\"BASE\"] = f1.item()\n",
    "    auprcs[\"BASE\"] = auprc\n",
    "    confusion_matrices = np.append(confusion_matrices, conf)\n",
    "\n",
    "    return accuracies, f1s, auprcs, confusion_matrices.reshape(3, 13, 13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also specify a function to save the confusion matrices obtained. This function takes as input the matrix to save, the model and dataset it corresponds to and if the data was oversampled or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to save the confusion matrices as heatmaps\n",
    "'''\n",
    "def save_confusion_matrix(confusion_matrix, model, dataset, oversampling):\n",
    "    # Define class names\n",
    "    class_names = ['Empty Square', 'White Pawn', 'White Knight', 'White Bishop', 'White Rook', 'White Queen', 'White King', \n",
    "                'Black Pawn', 'Black Knight', 'Black Bishop', 'Black Rook', 'Black Queen', 'Black King']\n",
    "    \n",
    "    # Define figure and axis size\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Create heatmap\n",
    "    heatmap = ax.imshow(confusion_matrix, cmap='Greens')\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = ax.figure.colorbar(heatmap, ax=ax)\n",
    "\n",
    "    # Set tick labels for x and y axis\n",
    "    ax.set_xticks(np.arange(len(class_names)))\n",
    "    ax.set_yticks(np.arange(len(class_names)))\n",
    "    ax.set_xticklabels(class_names, fontsize=10)\n",
    "    ax.set_yticklabels(class_names, fontsize=10)\n",
    "\n",
    "    # Rotate tick labels and set alignmentto have them appear vertically\n",
    "    ax.set_xticklabels(class_names, fontsize=10, rotation=90, ha='center')\n",
    "    plt.setp(ax.get_yticklabels(), rotation=0, ha='right')\n",
    "\n",
    "    #Loop over data dimensions and create text annotations\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            text = ax.text(j, i, '{:.1f}'.format(confusion_matrix[i, j]*100),\n",
    "                        ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "    #Set title and axis labels\n",
    "    if oversampling:\n",
    "        title = \"Confusion Matrix on \" + dataset + \" with oversampling (in %) - \"+model\n",
    "    else:\n",
    "        title = \"Confusion Matrix on \" + dataset + \" without oversampling (in %) - \"+model\n",
    "    \n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_xlabel(\"Predicted label\", fontsize=10)\n",
    "    ax.set_ylabel(\"True label\", fontsize=10)\n",
    "\n",
    "    \n",
    "    if oversampling:\n",
    "        # Save figure\n",
    "        plt.savefig(f\"../{model}/Confusion Matrices/Real Life/{dataset} Oversample.png\")\n",
    "    else:\n",
    "        # Save figure\n",
    "        plt.savefig(f\"../{model}/Confusion Matrices/Real Life/{dataset} No Oversample.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on computing the metrics we will save as JSON format or plot later (confusion matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionaries to store the results in\n",
    "accuracies = {}\n",
    "f1s = {}\n",
    "auprcs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the JSON files for the real life dataset with oversampling to the dictionaries created above \n",
    "# Save the confusion matrices as numpy arrays\n",
    "accuracies_real_life_oversample, f1s_real_life_oversample, confusion_matrices_real_life_oversample, auprcs_real_life_oversample = get_metrics(\"Real Life\", True)\n",
    "\n",
    "accuracies[\"Real Life Oversample\"] = accuracies_real_life_oversample\n",
    "f1s[\"Real Life Oversample\"] = f1s_real_life_oversample\n",
    "auprcs[\"Real Life Oversample\"] = auprcs_real_life_oversample\n",
    "save_confusion_matrix(confusion_matrices_real_life_oversample[0], \"DANN\", \"Real Life\", True)\n",
    "save_confusion_matrix(confusion_matrices_real_life_oversample[1], \"CORAL\", \"Real Life\", True)\n",
    "save_confusion_matrix(confusion_matrices_real_life_oversample[2], \"BASE\", \"Real Life\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the JSON files for the real life dataset without oversampling to the dictionaries created above\n",
    "# Save the confusion matrices as numpy arrays\n",
    "accuracies_real_life_no_oversample, f1s_real_life_no_oversample, confusion_matrices_real_life_no_oversample, auprcs_real_life_no_oversample = get_metrics(\"Real life\", False)\n",
    "\n",
    "accuracies[\"Real Life No Oversample\"] = accuracies_real_life_no_oversample\n",
    "f1s[\"Real Life No Oversample\"] = f1s_real_life_no_oversample\n",
    "auprcs[\"Real Life No Oversample\"] = auprcs_real_life_no_oversample\n",
    "save_confusion_matrix(confusion_matrices_real_life_no_oversample[0], \"DANN\", \"Real Life\", False)\n",
    "save_confusion_matrix(confusion_matrices_real_life_no_oversample[1], \"CORAL\", \"Real Life\", False)\n",
    "save_confusion_matrix(confusion_matrices_real_life_no_oversample[2], \"BASE\", \"Real Life\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the JSON files for the generated dataset with oversampling to the dictionaries created above\n",
    "# Save the confusion matrices as numpy arrays\n",
    "accuracies_generated_oversample, f1s_generated_oversample, confusion_matrices_generated_oversample, auprcs_generated_oversample = get_metrics(\"Generated\", True)\n",
    "\n",
    "accuracies[\"Generated Oversample\"] = accuracies_generated_oversample\n",
    "f1s[\"Generated Oversample\"] = f1s_generated_oversample\n",
    "auprcs[\"Generated Oversample\"] = auprcs_generated_oversample\n",
    "save_confusion_matrix(confusion_matrices_generated_oversample[0], \"DANN\", \"Generated\", True)\n",
    "save_confusion_matrix(confusion_matrices_generated_oversample[1], \"CORAL\", \"Generated\", True)\n",
    "save_confusion_matrix(confusion_matrices_generated_oversample[2], \"BASE\", \"Generated\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the JSON files for the generated dataset without oversampling to the dictionaries created above\n",
    "# Save the confusion matrices as numpy arrays\n",
    "accuracies_generated_no_oversample, f1s_generated_no_oversample, confusion_matrices_generated_no_oversample, auprcs_generated_no_oversample = get_metrics(\"Generated\", False)\n",
    "\n",
    "accuracies[\"Generated No Oversample\"] = accuracies_generated_no_oversample\n",
    "f1s[\"Generated No Oversample\"] = f1s_generated_no_oversample\n",
    "auprcs[\"Generated No Oversample\"] = auprcs_generated_no_oversample\n",
    "save_confusion_matrix(confusion_matrices_generated_no_oversample[0], \"DANN\", \"Generated\", False)\n",
    "save_confusion_matrix(confusion_matrices_generated_no_oversample[1], \"CORAL\", \"Generated\", False)\n",
    "save_confusion_matrix(confusion_matrices_generated_no_oversample[2], \"BASE\", \"Generated\", False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
