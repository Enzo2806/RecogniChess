{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is made to compare and evaluate each \"best model\" we obtained for DANN, CORAL and BASE model (VGG-16 trained on the generated data). All models are trained and tuned in their corresponding files (in the Models folder). This file evaluates them, and compares the best models obtained after hyperparameter tuning based on the result of multiple metrics: testing accuracy, f1-score, confusion matrix and average AUPRC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "import torchmetrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, auc\n",
    "import json\n",
    "\n",
    "# If VSCode doesn't pick up this import, see answer here: \n",
    "# https://stackoverflow.com/questions/65252074/import-path-to-own-script-could-not-be-resolved-pylance-reportmissingimports\n",
    "import sys\n",
    "sys.path.append(\"../../Datasets/\")\n",
    "from Custom_Dataset import * "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the device to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.has_mps:\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Training and Validation Curves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot all the metrics collected during the training of the models. These metrics were stored in the corresponding JSON files. We will only plot the metrics for the best models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_DANN(metrics_data, hyperparameters, sourcecolor=\"blue\", targetcolor=\"orange\"):\n",
    "    folder_path = f\"../DANN/Plots/lambda_{hyperparameters['lambda_DA']}_lr_{hyperparameters['learning_rate']}_gamma_{hyperparameters['gamma_focal_loss']}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Extract the relevant metrics from the JSON data\n",
    "    gen_training_discriminator_losses = metrics_data[\"gen_training_discriminator_losses/iteration\"]\n",
    "    gen_training_classifier_losses = metrics_data[\"gen_training_classifier_losses/iteration\"]\n",
    "    gen_training_feature_extractor_losses = metrics_data[\"gen_training_feature_extractor_losses/iteration\"]\n",
    "    real_training_discriminator_losses = metrics_data[\"real_training_discriminator_losses/iteration\"]\n",
    "    gen_training_accs = metrics_data[\"gen_training_accs/iteration\"]\n",
    "    gen_training_f1s = metrics_data[\"gen_training_f1s/iteration\"]\n",
    "    gen_validation_discriminator_losses = metrics_data[\"gen_validation_discriminator_losses/n_validation\"]\n",
    "    gen_validation_classifier_losses = metrics_data[\"gen_validation_classifier_losses/n_validation\"]\n",
    "    gen_validation_feature_extractor_losses = metrics_data[\"gen_validation_feature_extractor_losses/n_validation\"]\n",
    "    real_validation_discriminator_losses = metrics_data[\"real_validation_discriminator_losses/n_validation\"]\n",
    "    real_validation_classifier_losses = metrics_data[\"real_validation_classifier_losses/n_validation\"]\n",
    "    real_validation_feature_extractor_losses = metrics_data[\"real_validation_feature_extractor_losses/n_validation\"]\n",
    "    gen_validation_accs = metrics_data[\"gen_validation_accs/n_validation\"]\n",
    "    gen_validation_f1s = metrics_data[\"gen_validation_f1s/n_validation\"]\n",
    "    real_validation_accs = metrics_data[\"real_validation_accs/n_validation\"]\n",
    "    real_validation_f1s = metrics_data[\"real_validation_f1s/n_validation\"]\n",
    "    real_validation_accs_full = metrics_data[\"real_validation_accs_full/epoch\"]\n",
    "\n",
    "    # Compute the array used for the validation x axis\n",
    "    n_validation = hyperparameters[\"n_validation\"]\n",
    "    validation_x_axis = np.arange(0, len(gen_validation_discriminator_losses) * n_validation, n_validation)\n",
    "\n",
    "    # Plot gen_training_discriminator_losses \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(gen_training_discriminator_losses, color=sourcecolor)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Source Training Discriminator Losses\")\n",
    "    plt.savefig(f\"{folder_path}/source_training_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gen_training_classifier_losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(gen_training_classifier_losses, color=sourcecolor)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Source Training Classifier Losses\")\n",
    "    plt.savefig(f\"{folder_path}/source_training_classifier_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gen_training_feature_extractor_losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(gen_training_feature_extractor_losses, color=sourcecolor)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Source Training Feature Extractor Loss\")\n",
    "    plt.savefig(f\"{folder_path}/source_training_feature_extractor_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot real_training_discriminator_losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(real_training_discriminator_losses, color=sourcecolor)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Target Training Discriminator Loss\")\n",
    "    plt.savefig(f\"{folder_path}/target_training_discriminator_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gen_training_accs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(gen_training_accs, color=sourcecolor, label=\"Accuracy\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Source Training Accuracy\")\n",
    "    plt.savefig(f\"{folder_path}/source_training_accs.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gen_training_f1s\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(gen_training_f1s, color=sourcecolor, label=\"F1 score\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.title(\"Source Training F1 score\")\n",
    "    plt.savefig(f\"{folder_path}/source_training_f1s.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gen_validation_discriminator_losses and real_validation_discriminator_losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(validation_x_axis, gen_validation_discriminator_losses, color=sourcecolor, label=\"Source discriminator loss\")\n",
    "    plt.plot(validation_x_axis, real_validation_discriminator_losses, color=targetcolor, label=\"Target discriminator loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Validation Discriminator losses (Source and Target)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{folder_path}/validation_discriminator_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gen_validation_feature_extractor_losses and real_validation_feature_extractor_losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(validation_x_axis, gen_validation_feature_extractor_losses, color=sourcecolor, label=\"Source feature extractor loss\")\n",
    "    plt.plot(validation_x_axis, real_validation_feature_extractor_losses, color=targetcolor, label=\"Target feature extractor loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Validation Feature Extractor losses (Source and Target)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{folder_path}/validation_feature_extractor_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gen_validation_classifier_losses and real_validation_classifier_losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(validation_x_axis, gen_validation_classifier_losses, color=sourcecolor, label=\"Source classifier loss\")\n",
    "    plt.plot(validation_x_axis, real_validation_classifier_losses, color=targetcolor, label=\"Target classifier loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Validation Classifier losses (Source and Target)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{folder_path}/validation_classifier_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gen_validation_accs and real_validation_accs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(validation_x_axis, gen_validation_accs, color=sourcecolor, label=\"Source accuracy\")\n",
    "    plt.plot(validation_x_axis, real_validation_accs, color=targetcolor, label=\"Target accuracy\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Validation Accuracy (Source and Target)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{folder_path}/validation_accs.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot gen_validation_f1s and real_validation_f1s\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(validation_x_axis, gen_validation_f1s, color=sourcecolor, label=\"Source F1 score\")\n",
    "    plt.plot(validation_x_axis, real_validation_f1s, color=targetcolor, label=\"Target F1 score\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.title(\"Validation F1 score (Source and Target)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{folder_path}/validation_f1s.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot real_validation_accs_full\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(real_validation_accs_full, color=targetcolor, label=\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Target validation accuracy (Using the full dataset)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{folder_path}/target_validation_accs_full.png\", dpi=500)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JSON files and plot the metrics\n",
    "folder_path = \"../../Models/DANN/HP tuning results/\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check file format\n",
    "    if file_name.endswith(\".json\"):\n",
    "        # Get the file path\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # Open the JSON file\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            hyperparameters = json_data[\"hyperparameters\"]\n",
    "            metrics_data = json_data[\"metrics\"]\n",
    "            plot_metrics_DANN(metrics_data, hyperparameters, sourcecolor=\"saddlebrown\", targetcolor=\"orange\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_CORAL(metrics_data, hyperparameters, sourcecolor=\"blue\", targetcolor=\"orange\"):\n",
    "    folder_path = f\"../CORAL/Plots/lr_{hyperparameters['learning_rate']}_lambdamax_{hyperparameters['lambda_max_DA']}_gamma_{hyperparameters['gamma_focal_loss']}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # Extract the relevant metrics from the JSON data\n",
    "    training_losses = metrics_data[\"training_total_losses/iteration\"]\n",
    "    coral_losses = metrics_data[\"training_coral_losses/iteration\"]\n",
    "    gen_classification_losses = metrics_data[\"gen_training_classification_losses/iteration\"]\n",
    "    gen_accs = metrics_data[\"gen_training_accs/iteration\"]\n",
    "    gen_f1s = metrics_data[\"gen_training_f1s/iteration\"]\n",
    "    gen_val_accs = metrics_data[\"gen_validation_accs/n_validation\"]\n",
    "    real_val_accs = metrics_data[\"real_validation_accs/n_validation\"]\n",
    "    gen_val_f1s = metrics_data[\"gen_validation_f1s/n_validation\"]\n",
    "    real_val_f1s = metrics_data[\"real_validation_f1s/n_validation\"]\n",
    "    val_losses = metrics_data[\"validation_total_losses_using_gen_classification_losses/n_validation\"]\n",
    "    coral_val_losses = metrics_data[\"validation_CORAL_losses/n_validation\"]\n",
    "    gen_val_classification_losses = metrics_data[\"gen_validation_classification_losses/n_validation\"]\n",
    "    real_val_classification_losses = metrics_data[\"real_validation_classification_losses/n_validation\"]\n",
    "    real_val_accs_full = metrics_data[\"real_validation_accs_full/epoch\"]\n",
    "\n",
    "    # Compute the array used for the validation x axis\n",
    "    n_validation = hyperparameters[\"n_validation\"]\n",
    "    validation_x_axis = np.arange(0, len(val_losses) * n_validation, n_validation)\n",
    "\n",
    "    # Create plots for training_losses and coral_losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_losses, label='Training Total Losses', color=sourcecolor)\n",
    "    plt.plot(coral_losses, label='Training Coral Losses', color=targetcolor)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Losses')\n",
    "    plt.savefig(f\"{folder_path}/training_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_training_classification_losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gen_classification_losses, color=sourcecolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Source Training Classification Losses')\n",
    "    plt.savefig(f\"{folder_path}/source_training_classification_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_training_accs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gen_accs, color=sourcecolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Source Training Accs')\n",
    "    plt.savefig(f\"{folder_path}/source_training_accs.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_training_f1s\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gen_f1s, color=sourcecolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Source Training F1s')\n",
    "    plt.savefig(f\"{folder_path}/source_training_f1s.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_validation_accs and real_validation_accs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(validation_x_axis, gen_val_accs, label='Source', color=sourcecolor)\n",
    "    plt.plot(validation_x_axis, real_val_accs, label='Target', color=targetcolor)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracies (Source vs. Target Domain)')\n",
    "    plt.savefig(f\"{folder_path}/validation_accs.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_validation_f1s and real_validation_f1s\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(validation_x_axis, gen_val_f1s, label='Source', color=sourcecolor)\n",
    "    plt.plot(validation_x_axis, real_val_f1s, label='Target', color=targetcolor)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Validation F1 Scores (Source vs. Target Domain)')\n",
    "    plt.savefig(f\"{folder_path}/validation_f1s.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for val_losses and coral_val_losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(validation_x_axis, val_losses, label='Validation Total Losses', color=sourcecolor)\n",
    "    plt.plot(validation_x_axis, coral_val_losses, label='Validation CORAL Losses', color=targetcolor)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Losses')\n",
    "    plt.savefig(f\"{folder_path}/validation_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_val_classification_losses and real_val_classification_losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(validation_x_axis, gen_val_classification_losses, label='Source', color=sourcecolor)\n",
    "    plt.plot(validation_x_axis, real_val_classification_losses, label='Target', color=targetcolor)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Classification Losses (Source vs Target Domain)')\n",
    "    plt.savefig(f\"{folder_path}/classification_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for real_val_accs_full\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(real_val_accs_full, label='Target', color=targetcolor)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Target validation accuracy (Using the full dataset)')\n",
    "    plt.savefig(f\"{folder_path}/target_val_accs_full.png\", dpi=500)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JSON files and plot the metrics\n",
    "folder_path = \"../../Models/CORAL/HP tuning results/\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check file format\n",
    "    if file_name.endswith(\".json\"):\n",
    "        # Get the file path\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # Open the JSON file\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            hyperparameters = json_data[\"hyperparameters\"]\n",
    "            metrics_data = json_data[\"metrics\"]\n",
    "            plot_metrics_CORAL(metrics_data, hyperparameters, sourcecolor=\"saddlebrown\", targetcolor=\"orange\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_BASE_Source(metrics_data, hyperparameters, sourcecolor=\"blue\", targetcolor=\"orange\"):\n",
    "    folder_path = f\"../BASE Source/Plots/gamma_{hyperparameters['gamma']}_dropout_{hyperparameters['dropout_rate']}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Extract the relevant metrics from the JSON data\n",
    "    gen_training_losses = metrics_data[\"gen_training_losses/iteration\"]\n",
    "    gen_training_accs = metrics_data[\"gen_training_accs/iteration\"]\n",
    "    gen_training_f1s = metrics_data[\"gen_training_f1s/iteration\"]\n",
    "    gen_validation_accs = metrics_data[\"gen_validation_accs/n_validation\"]\n",
    "    real_validation_accs = metrics_data[\"real_validation_accs/n_validation\"]\n",
    "    gen_validation_f1s = metrics_data[\"gen_validation_f1s/n_validation\"]\n",
    "    real_validation_f1s = metrics_data[\"real_validation_f1s/n_validation\"]\n",
    "    real_validation_accs_full = metrics_data[\"real_validation_accs_full/epoch\"]\n",
    "\n",
    "    # Compute the array used for the validation x axis\n",
    "    n_validation = hyperparameters[\"n_validation\"]\n",
    "    validation_x_axis = np.arange(0, len(gen_validation_accs) * n_validation, n_validation)\n",
    "\n",
    "    # Create plots for gen_training_losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gen_training_losses, color=sourcecolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Source Training Losses')\n",
    "    plt.savefig(f\"{folder_path}/source_training_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_training_accs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gen_training_accs, color=sourcecolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Source Training Accuracies')\n",
    "    plt.savefig(f\"{folder_path}/gen_training_accs.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_training_f1s\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gen_training_f1s, color=sourcecolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Source Training F1 Scores')\n",
    "    plt.savefig(f\"{folder_path}/source_training_f1s.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_validation_accs and real_validation_accs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(validation_x_axis, gen_validation_accs, label='Source', color=sourcecolor)\n",
    "    plt.plot(validation_x_axis,  real_validation_accs, label='Target', color=targetcolor)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracies (Source vs. Target Domain)')\n",
    "    plt.savefig(f\"{folder_path}/validation_accs.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for gen_validation_f1s and real_validation_f1s\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(validation_x_axis, gen_validation_f1s, label='Source', color=sourcecolor)\n",
    "    plt.plot(validation_x_axis, real_validation_f1s, label='Target', color=targetcolor)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Validation F1 Scores (Source vs. Target Domain)')\n",
    "    plt.savefig(f\"{folder_path}/validation_f1s.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for real_validation_accs_full\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(real_validation_accs_full, label='Target', color=targetcolor)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Target validation accuracy (Using the full dataset)')\n",
    "    plt.savefig(f\"{folder_path}/target_validation_accs_full.png\", dpi=500)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JSON files and plot the metrics for the Base model trained on the source domain\n",
    "folder_path = \"../../Models/BASE/TRAIN_GEN/HP tuning results/\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check file format\n",
    "    if file_name.endswith(\".json\"):\n",
    "        # Get the file path\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # Open the JSON file\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            hyperparameters = json_data[\"hyperparameters\"]\n",
    "            metrics_data = json_data[\"metrics\"]\n",
    "            plot_metrics_BASE_Source(metrics_data, hyperparameters, sourcecolor=\"saddlebrown\", targetcolor=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_BASE_Target(metrics_data, hyperparameters, targetcolor=\"orange\"):\n",
    "    folder_path = f\"../BASE Target/Plots/gamma_{hyperparameters['gamma']}_dropout_{hyperparameters['dropout_rate']}\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # Extract the relevant metrics from the JSON data\n",
    "    real_training_losses = metrics_data[\"real_training_losses/iteration\"]\n",
    "    real_training_accs = metrics_data[\"real_training_accs/iteration\"]\n",
    "    real_training_f1s = metrics_data[\"real_training_f1s/iteration\"]\n",
    "    real_validation_losses = metrics_data[\"real_validation_losses/n_validation\"]\n",
    "    real_validation_accs = metrics_data[\"real_validation_accs/n_validation\"]\n",
    "    real_validation_f1s = metrics_data[\"real_validation_f1s/n_validation\"]\n",
    "    real_validation_accs_full = metrics_data[\"real_validation_accs_full/epoch\"]\n",
    "\n",
    "    # Compute the array used for the validation x axis\n",
    "    n_validation = hyperparameters[\"n_validation\"]\n",
    "    validation_x_axis = np.arange(0, len(real_validation_losses) * n_validation, n_validation)\n",
    "\n",
    "    # Create plots for real_training_losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(real_training_losses, color=targetcolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Target Training Losses')\n",
    "    plt.savefig(f\"{folder_path}/target_training_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for real_training_accs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(real_training_accs, color=targetcolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Target Training Accuracies')\n",
    "    plt.savefig(f\"{folder_path}/target_training_accs.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for real_training_f1s\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(real_training_f1s, color=targetcolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Target Training F1 Scores')\n",
    "    plt.savefig(f\"{folder_path}/target_training_f1s.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for real_validation_losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(validation_x_axis, real_validation_losses, color=targetcolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Target Validation Losses')\n",
    "    plt.savefig(f\"{folder_path}/target_validation_losses.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for real_validation_accs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(validation_x_axis, real_validation_accs, color=targetcolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Target Validation Accuracies')\n",
    "    plt.savefig(f\"{folder_path}/real_validation_accs.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for real_validation_f1s\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(validation_x_axis, real_validation_f1s, color=targetcolor)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Target Validation F1 Scores')\n",
    "    plt.savefig(f\"{folder_path}/target_validation_f1s.png\", dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "    # Create plots for real_validation_accs_full\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(real_validation_accs_full, label='Target', color=targetcolor)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Target validation accuracy (Using the full dataset)')\n",
    "    plt.savefig(f\"{folder_path}/target_validation_accs_full.png\", dpi=500)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JSON files and plot the metrics for the Base model trained on the target domain\n",
    "folder_path = \"../../Models/BASE/TRAIN_VAL_REAL/HP tuning results/\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check file format\n",
    "    if file_name.endswith(\".json\"):\n",
    "        # Get the file path\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # Open the JSON file\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            hyperparameters = json_data[\"hyperparameters\"]\n",
    "            metrics_data = json_data[\"metrics\"]\n",
    "            plot_metrics_BASE_Target(metrics_data, hyperparameters, targetcolor=\"orange\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the classes for each model. The base model and CORAL model have one class, while the DANN has two classes: one for the feature extractor and one for the classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=4608, num_classes=13):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        c = self.layer(h)\n",
    "        return c\n",
    "    \n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "        Feature Extractor\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        # Import the VGG16 model\n",
    "        self.conv = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "\n",
    "        # Freeze all the weights in modules 0 up-to and including 25\n",
    "        for param in self.conv[:25].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoralModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=13):\n",
    "        \n",
    "        super(CoralModel, self).__init__()\n",
    "        \n",
    "        # Define the layers of the model\n",
    "        self.features = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4608, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Freeze all the weights in modules 0 up-to and including 25\n",
    "        for param in self.features[:25].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.features(x)\n",
    "        h = torch.flatten(h, 1)\n",
    "        output = self.classifier(h)\n",
    "        return h, output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class for the Base Model\n",
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=13, dropout_rate=0.5):\n",
    "        \n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        # Define the layers of the model\n",
    "        self.features = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4608, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Set the features to not require gradients\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the models we saved as the best ones for each model type (DANN, CORAL, and Base Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DANN_path = \"../../Models/DANN/\"\n",
    "DANN_C_path = DANN_path + \"best_DANN_C_model.ckpt\"\n",
    "DANN_F_path = DANN_path + \"best_DANN_F_model.ckpt\"\n",
    "CORAL_path = \"../../Models/CORAL/best_CORAL_model.ckpt\"\n",
    "BASE_Source_path = \"../../Models/BASE/TRAIN_GEN/best_BASE_TRAIN_GEN_model.ckpt\"\n",
    "BASE_Target_path = \"../../Models/BASE/TRAIN_VAL_REAL/best_BASE_TRAIN_VAL_REAL_model.ckpt\"\n",
    "\n",
    "# Instantiate the classifier model object for the dann model\n",
    "DANN_C = Classifier().to(DEVICE)\n",
    "# Load the state dictionary from the checkpoint file\n",
    "state_dict = torch.load(DANN_C_path, map_location=torch.device(DEVICE))\n",
    "# Load the state dictionary into the model\n",
    "DANN_C.load_state_dict(state_dict)\n",
    "# Put the model into evaluation mode\n",
    "DANN_C.eval()\n",
    "\n",
    "# Instantiate the feature extractor model object for the dann model\n",
    "DANN_F = FeatureExtractor().to(DEVICE)\n",
    "# Load the state dictionary from the checkpoint file\n",
    "state_dict = torch.load(DANN_F_path, map_location=torch.device(DEVICE))\n",
    "# Load the state dictionary into the model\n",
    "DANN_F.load_state_dict(state_dict)\n",
    "# Put the model into evaluation mode\n",
    "DANN_F.eval()\n",
    "\n",
    "# Instantiate the model object for the coral model\n",
    "coral_model = CoralModel().to(DEVICE)\n",
    "# Load the state dictionary from the checkpoint file\n",
    "state_dict = torch.load(CORAL_path, map_location=torch.device(DEVICE))\n",
    "# Load the state dictionary into the model\n",
    "coral_model.load_state_dict(state_dict)\n",
    "# Put the model into evaluation mode\n",
    "coral_model.eval()\n",
    "\n",
    "# Instantiate the model object for the base model trained on the source dataset\n",
    "base_model_source = BaseModel().to(DEVICE)\n",
    "# Load the state dictionary from the checkpoint file\n",
    "state_dict = torch.load(BASE_Source_path, map_location=torch.device(DEVICE))\n",
    "# Load the state dictionary into the model\n",
    "base_model_source.load_state_dict(state_dict)\n",
    "# Put the model into evaluation mode\n",
    "base_model_source.eval()\n",
    "\n",
    "# Instantiate the model object for the base model trained on the target dataset\n",
    "base_model_target = BaseModel().to(DEVICE)\n",
    "# Load the state dictionary from the checkpoint file\n",
    "state_dict = torch.load(BASE_Target_path, map_location=torch.device(DEVICE))\n",
    "# Load the state dictionary into the model\n",
    "base_model_target.load_state_dict(state_dict)\n",
    "# Put the model into evaluation mode\n",
    "base_model_target.eval()\n",
    "\n",
    "# To not show the Base architecture\n",
    "print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and metric collection functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the performance of the best models using the testing accuracy, f1-score, confusion matrix and average AUPRC metrics. \n",
    "\n",
    "* Specifically, for the testing accuracy, we will use the balanced accuracy score, which is similar to regular classification accuracy, but it takes into account the frequency of each class. The balanced accuracy score will be computed using on both the real-life dataset and the generated dataset, however, our ultimate goal is to predict labels for real-life images.\n",
    "\n",
    "* As a reminder, the f1-score is a performance metric that provides a balance between the precision and recall of the model. The f1-score ranges from 0 to 1, with a higher score indicating a better performance of the model. The equation for f1-score is: $$ f1 = \\frac{2 (precision * recall)}{precision + recall}$$ where $precision$ is the number of true positives (correctly predicted positive samples) divided by the total number of predicted positive samples, and $recall$ is the number of true positives divided by the total number of actual positive samples. In other words, f1-score takes into account both the model's ability to correctly identify positive samples (recall) and its tendency to not mislabel negative samples as positive (precision). \n",
    "\n",
    "* The confusion matrix compares the predicted class labels with the true class labels and counts the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) for each class. It can help identify which classes the model is having difficulty classifying correctly, and whihc classes it can predict with ease.\n",
    "\n",
    "* Finally, the AUPRC measures the overall quality of the model's predictions, taking into account both precision and recall across all possible classification thresholds. A high average AUPRC indicates that the model has a good balance of precision and recall across all possible classification thresholds, and is able to accurately distinguish between positive and negative cases. Conversely, a low average AUPRC indicates poor performance, and suggests that the model is making many incorrect predictions or missing many true positive cases.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the balanced accuracy function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the balanced accuracy function\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=13, average=\"weighted\").to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the final testing accuracies of each model (DANN, CORAL, and Base model), we evaluate their performance on both the real-life dataset and the generated dataset. Additionally, we analyze the metrics with and without oversampling. For this purpose, we use a function that takes the model(s), batch size, dataset and oversampling boolean as input to compute the testing accuracy, f1-score, confusion matrix and average AUPRC. \n",
    "\n",
    "It's important to note that the most important metrics we will use for comparison are the one on the real-life dataset without oversampling (so a classic distribution of chess boards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing accuracy evaluation function\n",
    "def evaluate(model, batch_size, dataset, model2=None, oversampling=True, coral=False):\n",
    "    # We use our custom dataset and loader defined in the \"Datasets\" folder.\n",
    "    test_dataset = CustomDataset(dataset, \"test\", balance=oversampling)\n",
    "    loader = get_loader(test_dataset, batch_size=batch_size, balance=oversampling)\n",
    "\n",
    "    # Makes sur emodel is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Set accumulated accuracy to 0\n",
    "    acc = 0\n",
    "\n",
    "    y_true = [] # Ground truth labels\n",
    "    y_hat = [] # Predicted labels\n",
    "\n",
    "    # Remove grad\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            # Move the data to the device\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            if coral:\n",
    "                # Forward pass\n",
    "                _, y_pred_prob = model(images)\n",
    "            else:\n",
    "                # Forward pass\n",
    "                y_pred_prob = model(images)\n",
    "\n",
    "            # If there is a second model \n",
    "            # (for DANN we first go through the Feature extractor and then the Classifier)\n",
    "            if model2:\n",
    "                y_pred_prob = model2(y_pred_prob)\n",
    "\n",
    "            y_pred = torch.argmax(y_pred_prob, dim=1)\n",
    "\n",
    "            # Compute the metrics\n",
    "            acc +=  accuracy(y_pred, labels)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_hat.extend(y_pred.cpu().numpy())\n",
    "\n",
    "        # Compute the average accuracy\n",
    "        final_accuracy = acc / len(loader)\n",
    "        \n",
    "        # Compute the f1 score\n",
    "        f1 = f1_score(y_true, y_hat, average='weighted')\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_hat, normalize='true')\n",
    "\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_hat = np.asarray(y_hat)\n",
    "\n",
    "        n_classes = 13\n",
    "        from sklearn.preprocessing import LabelBinarizer\n",
    "        lb = LabelBinarizer()\n",
    "        y_true_binary = lb.fit_transform(y_true)\n",
    "        y_hat_binary = lb.fit_transform(y_hat)\n",
    "\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        for i in range(n_classes):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_true_binary[:, i], y_hat_binary[:, i])\n",
    "            \n",
    "        auprc = dict()\n",
    "        for i in range(n_classes):\n",
    "            auprc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "        average_auprc = np.mean(list(auprc.values()))\n",
    "\n",
    "    # Return all metrics computed\n",
    "    return final_accuracy, f1, cm, average_auprc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a last function \"get_metrics\" to obtain all the metrics we defined above for all models depending on the dataset and if the data is oversampled or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the batch size to 32 for all accuracy computations.\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to get the testing accuracy, f1 score, confusion matrix and average AUPRC for all DANN, CORAL and Base models.\n",
    "It takes as input:\n",
    "- the dataset to evaluate the metrics on (\"Real Life\" or \"Generated\")\n",
    "- if oversampling must be performed\n",
    "'''\n",
    "def get_metrics(dataset, oversampling):\n",
    "    # Create empty dictionaries to store the results in\n",
    "    # We will save these dictionaries in JSON files\n",
    "    accuracies = {}\n",
    "    f1s = {}\n",
    "    auprcs = {}\n",
    "\n",
    "    # We will not save the confusion matrix as JSON, but we will save it as a numpy array\n",
    "    # We will direclty save it using matplotlib\n",
    "    confusion_matrices = np.empty([0]) \n",
    "    \n",
    "    # Evaluate the DANN model\n",
    "    accuracy, f1, conf, auprc= evaluate(DANN_F, batch_size, dataset, DANN_C, oversampling=oversampling)\n",
    "    # Append results\n",
    "    accuracies[\"DANN\"] = accuracy.item()\n",
    "    f1s[\"DANN\"] = f1.item()\n",
    "    auprcs[\"DANN\"] = auprc\n",
    "    confusion_matrices = np.append(confusion_matrices, conf)\n",
    "\n",
    "    # Evaluate the CORAL model\n",
    "    accuracy, f1, conf, auprc= evaluate(coral_model, batch_size, dataset, oversampling=oversampling, coral=True)\n",
    "    # Append results\n",
    "    accuracies[\"CORAL\"] = accuracy.item()\n",
    "    f1s[\"CORAL\"] = f1.item()\n",
    "    auprcs[\"CORAL\"] = auprc\n",
    "    confusion_matrices = np.append(confusion_matrices, conf)\n",
    "\n",
    "    # Evaluate the Base model trained on the source dataset\n",
    "    accuracy, f1, conf, auprc = evaluate(base_model_source, batch_size, dataset, oversampling=oversampling)\n",
    "    # Append results\n",
    "    accuracies[\"BASE Source\"] = accuracy.item()\n",
    "    f1s[\"BASE Source\"] = f1.item()\n",
    "    auprcs[\"BASE Source\"] = auprc\n",
    "    confusion_matrices = np.append(confusion_matrices, conf)\n",
    "\n",
    "    # Evaluate the Base model trained on the target dataset\n",
    "    accuracy, f1, conf, auprc = evaluate(base_model_target, batch_size, dataset, oversampling=oversampling)\n",
    "    # Append results\n",
    "    accuracies[\"BASE Target\"] = accuracy.item()\n",
    "    f1s[\"BASE Target\"] = f1.item()\n",
    "    auprcs[\"BASE Target\"] = auprc\n",
    "    confusion_matrices = np.append(confusion_matrices, conf)\n",
    "\n",
    "    return accuracies, f1s, auprcs, confusion_matrices.reshape(4, 13, 13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also specify a function to save the confusion matrices obtained. This function takes as input the matrix to save, the model and dataset it corresponds to and if the data was oversampled or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to save the confusion matrices as heatmaps\n",
    "'''\n",
    "def save_confusion_matrix(confusion_matrix, model, dataset, oversampling):\n",
    "    # Define class names\n",
    "    class_names = ['Empty Square', 'White Pawn', 'White Knight', 'White Bishop', 'White Rook', 'White Queen', 'White King', \n",
    "                'Black Pawn', 'Black Knight', 'Black Bishop', 'Black Rook', 'Black Queen', 'Black King']\n",
    "    \n",
    "    # Define figure and axis size\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Create heatmap\n",
    "    heatmap = ax.imshow(confusion_matrix, cmap='Greens')\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = ax.figure.colorbar(heatmap, ax=ax)\n",
    "\n",
    "    # Set tick labels for x and y axis\n",
    "    ax.set_xticks(np.arange(len(class_names)))\n",
    "    ax.set_yticks(np.arange(len(class_names)))\n",
    "    ax.set_xticklabels(class_names, fontsize=10)\n",
    "    ax.set_yticklabels(class_names, fontsize=10)\n",
    "\n",
    "    # Rotate tick labels and set alignmentto have them appear vertically\n",
    "    ax.set_xticklabels(class_names, fontsize=10, rotation=90, ha='center')\n",
    "    plt.setp(ax.get_yticklabels(), rotation=0, ha='right')\n",
    "\n",
    "    #Loop over data dimensions and create text annotations\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            text = ax.text(j, i, '{:.1f}'.format(confusion_matrix[i, j]*100),\n",
    "                        ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "    #Set title and axis labels\n",
    "    if oversampling:\n",
    "        title = \"Confusion Matrix on \" + dataset + \" with oversampling (in %) - \"+model\n",
    "    else:\n",
    "        title = \"Confusion Matrix on \" + dataset + \" without oversampling (in %) - \"+model\n",
    "    \n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_xlabel(\"Predicted label\", fontsize=10)\n",
    "    ax.set_ylabel(\"True label\", fontsize=10)\n",
    "\n",
    "    \n",
    "    if oversampling:\n",
    "        # Save figure\n",
    "        plt.savefig(f\"../{model}/Confusion Matrices/{dataset} Oversample.png\", dpi=500)\n",
    "    else:\n",
    "        # Save figure\n",
    "        plt.savefig(f\"../{model}/Confusion Matrices/{dataset} No Oversample.png\", dpi=500)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on computing the metrics we will save as JSON format or plot later (confusion matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionaries to store the results in\n",
    "accuracies = {}\n",
    "f1s = {}\n",
    "auprcs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the JSON files for the real life dataset with oversampling to the dictionaries created above \n",
    "# Save the confusion matrices as numpy arrays\n",
    "accuracies_real_life_oversample, f1s_real_life_oversample, auprcs_real_life_oversample, confusion_matrices_real_life_oversample = get_metrics(\"Real Life\", True)\n",
    "\n",
    "accuracies[\"Real Life Oversample\"] = accuracies_real_life_oversample\n",
    "f1s[\"Real Life Oversample\"] = f1s_real_life_oversample\n",
    "auprcs[\"Real Life Oversample\"] = auprcs_real_life_oversample\n",
    "save_confusion_matrix(confusion_matrices_real_life_oversample[0], \"DANN\", \"Real Life\", True)\n",
    "save_confusion_matrix(confusion_matrices_real_life_oversample[1], \"CORAL\", \"Real Life\", True)\n",
    "save_confusion_matrix(confusion_matrices_real_life_oversample[2], \"BASE Source\", \"Real Life\", True)\n",
    "save_confusion_matrix(confusion_matrices_real_life_oversample[3], \"BASE Target\", \"Real Life\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the JSON files for the real life dataset without oversampling to the dictionaries created above\n",
    "# Save the confusion matrices as numpy arrays\n",
    "accuracies_real_life_no_oversample, f1s_real_life_no_oversample, auprcs_real_life_no_oversample, confusion_matrices_real_life_no_oversample = get_metrics(\"Real Life\", False)\n",
    "\n",
    "accuracies[\"Real Life No Oversample\"] = accuracies_real_life_no_oversample\n",
    "f1s[\"Real Life No Oversample\"] = f1s_real_life_no_oversample\n",
    "auprcs[\"Real Life No Oversample\"] = auprcs_real_life_no_oversample\n",
    "save_confusion_matrix(confusion_matrices_real_life_no_oversample[0], \"DANN\", \"Real Life\", False)\n",
    "save_confusion_matrix(confusion_matrices_real_life_no_oversample[1], \"CORAL\", \"Real Life\", False)\n",
    "save_confusion_matrix(confusion_matrices_real_life_no_oversample[2], \"BASE Source\", \"Real Life\", False)\n",
    "save_confusion_matrix(confusion_matrices_real_life_no_oversample[3], \"BASE Target\", \"Real Life\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the JSON files for the generated dataset with oversampling to the dictionaries created above\n",
    "# Save the confusion matrices as numpy arrays\n",
    "accuracies_generated_oversample, f1s_generated_oversample, auprcs_generated_oversample, confusion_matrices_generated_oversample= get_metrics(\"Generated\", True)\n",
    "\n",
    "accuracies[\"Generated Oversample\"] = accuracies_generated_oversample\n",
    "f1s[\"Generated Oversample\"] = f1s_generated_oversample\n",
    "auprcs[\"Generated Oversample\"] = auprcs_generated_oversample\n",
    "save_confusion_matrix(confusion_matrices_generated_oversample[0], \"DANN\", \"Generated\", True)\n",
    "save_confusion_matrix(confusion_matrices_generated_oversample[1], \"CORAL\", \"Generated\", True)\n",
    "save_confusion_matrix(confusion_matrices_generated_oversample[2], \"BASE Source\", \"Generated\", True)\n",
    "save_confusion_matrix(confusion_matrices_generated_oversample[3], \"BASE Target\", \"Generated\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the JSON files for the generated dataset without oversampling to the dictionaries created above\n",
    "# Save the confusion matrices as numpy arrays\n",
    "accuracies_generated_no_oversample, f1s_generated_no_oversample, auprcs_generated_no_oversample, confusion_matrices_generated_no_oversample = get_metrics(\"Generated\", False)\n",
    "\n",
    "accuracies[\"Generated No Oversample\"] = accuracies_generated_no_oversample\n",
    "f1s[\"Generated No Oversample\"] = f1s_generated_no_oversample\n",
    "auprcs[\"Generated No Oversample\"] = auprcs_generated_no_oversample\n",
    "save_confusion_matrix(confusion_matrices_generated_no_oversample[0], \"DANN\", \"Generated\", False)\n",
    "save_confusion_matrix(confusion_matrices_generated_no_oversample[1], \"CORAL\", \"Generated\", False)\n",
    "save_confusion_matrix(confusion_matrices_generated_no_oversample[2], \"BASE Source\", \"Generated\", False)\n",
    "save_confusion_matrix(confusion_matrices_generated_no_oversample[3], \"BASE Target\", \"Generated\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionaries as JSON files\n",
    "with open('../Testing Accuracies.json', 'w') as fp:\n",
    "    json.dump(accuracies, fp)\n",
    "\n",
    "with open('../Testing F1 Scores.json', 'w') as fp:\n",
    "    json.dump(f1s, fp)\n",
    "\n",
    "with open('../Testing AUPRC.json', 'w') as fp:\n",
    "    json.dump(auprcs, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
