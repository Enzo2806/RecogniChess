{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b950cda7",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "144fa7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now load the dependencies\n",
    "%matplotlib inline \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision\n",
    "import torchsummary\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6211a13b",
   "metadata": {},
   "source": [
    "We can start by setting a seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356ddda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x183633c7d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8c3d84a",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Copy code in here when Enzo is done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b93703ce",
   "metadata": {},
   "source": [
    "# Hyperparameter choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b259e4e1",
   "metadata": {},
   "source": [
    "We create a cell to hold the hyperparameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6ce00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "gamma_focal_loss = 2\n",
    "n_loss = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "385914aa",
   "metadata": {},
   "source": [
    "We can now create our dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec326c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and dataloaders\n",
    "gen_train_loader = None\n",
    "\n",
    "gen_val_loader = None\n",
    "\n",
    "gen_test_loader = None\n",
    "\n",
    "real_train_loader = None\n",
    "\n",
    "real_val_loader = None\n",
    "\n",
    "real_test_loader = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e60fc99",
   "metadata": {},
   "source": [
    "# Model implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "868ef8b6",
   "metadata": {},
   "source": [
    "We can start by loading a pre-trained VGG16 model without the classification layers towards the end (Only the feature extractor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a976a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8433f526",
   "metadata": {},
   "source": [
    "We can now visualize its layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9614fd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "018e9568",
   "metadata": {},
   "source": [
    "Because we are looking for a pre-trained feature extractor here, we decide to only use the features part and freeze its weights. We can then add a few subsequent layers to fine tune predictions. We can thus define the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a051bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=13):\n",
    "        \n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        # Define the layers of the model\n",
    "        self.features = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4608, 2304),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2304, 1152),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1152, 576),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(576, num_classes)\n",
    "        )\n",
    "\n",
    "        # Set the features to not require gradients\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7348424",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3c49179",
   "metadata": {},
   "source": [
    "We can start by finding the device to use for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1237640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9b60888",
   "metadata": {},
   "source": [
    "We can then initialize and visualize our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "474bf49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=4608, out_features=2304, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=2304, out_features=1152, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=1152, out_features=576, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=576, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basemodel = BaseModel().to(DEVICE)\n",
    "basemodel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61cb4352",
   "metadata": {},
   "source": [
    "We can then go ahead and define the loss function we will be using. Because we will opt for a balanced focal loss instead of a regular cross entropy loss which gives more importance to the classes that are harder to classify. We thus implement the focal loss defined by the following formula:\n",
    "<center><img src=\"focal loss.png\"></center>\n",
    "\n",
    "where gamma is a tunable hyperparameter. We also further add an alpha term to handle class imbalance, making our loss function a class-balanced focal loss, as shown in https://github.com/AdeelH/pytorch-multi-class-focal-loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54555b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_loss = torch.hub.load(\n",
    "\t'adeelh/pytorch-multi-class-focal-loss',\n",
    "\tmodel='FocalLoss',\n",
    "\talpha=torch.tensor([.75, .25]), # TODO: Change the alpha values to represent class proportions\n",
    "\tgamma=gamma_focal_loss,\n",
    "\treduction='mean',\n",
    "\tforce_reload=False,\n",
    "    verbose = False\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "057bb473",
   "metadata": {},
   "source": [
    "Finally, we need an accuracy metric to tune the hyperparameters of the model. We will opt for a balanced accuracy score, which is just regular classification accuracy but adapted to weigh each class by its frequency. We can use the scikit-learn's https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html implementation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2047f74c",
   "metadata": {},
   "source": [
    "We can now define our optimizers. We opt for ADAM gradient descent since it is an industry standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42361516",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(basemodel.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e0fb873",
   "metadata": {},
   "source": [
    "We can now proceed to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e90133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bfdd6b4",
   "metadata": {},
   "source": [
    "# Extra code to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b933d",
   "metadata": {},
   "source": [
    "We will also define Focal Loss as our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bee1f90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3693, 358, 8, 9, 9, 4, 9, 400, 7, 8, 6, 2, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loss()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from balanced_loss import Loss\n",
    "\n",
    "# outputs and labels\n",
    "\n",
    "num_samples = len(y_generated)\n",
    "\n",
    "# percentage_per_class = [73.75, 7.19, 1.41, 1.56, 1.56, 0.78, 1.56, 6.41, 1.25, 1.41, 1.09, 0.47, 1.56]\n",
    "# samples_per_class = [int(num_samples * percentage / 100) for percentage in percentage_per_class]\n",
    "\n",
    "samples_per_class = [236400, 22944, 564, 624, 624, 312, 624, 25632, 500, 564, 436, 188, 624]\n",
    "samples_per_class = [int(sample_count/len(y_generated)) for sample_count in samples_per_class]\n",
    "\n",
    "print(samples_per_class)\n",
    "\n",
    "# class-balanced focal loss\n",
    "fLoss = Loss(\n",
    "    loss_type = \"focal_loss\",\n",
    "    beta = 2,\n",
    "    fl_gamma = 3,\n",
    "    samples_per_class = samples_per_class,\n",
    "    class_balanced = False\n",
    ")\n",
    "\n",
    "fLoss.to(device) # We move the loss function to device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd7ed8",
   "metadata": {},
   "source": [
    "## We got it running for EX0000. Let's try running the first 10 np arrays [EX0000-EX0009] and adding epochs as well (NOTE, for now this only works on CPU, not CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62da88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_examples = 10\n",
    "X_generated = np.empty([64*num_training_examples, 3, 130, 130])\n",
    "\n",
    "for i in range(num_training_examples): \n",
    "    X_generated[i*64:(i+1)*64] = np.load(\"C:/Users/usuario/RecogniChess/Data Generation/Pre Processed Data Generated/Images/EX_%04d.npy.npz\" % i, allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_generated = np.empty([64*num_training_examples]) # Create empty array to store the labels\n",
    "\n",
    "# Loop through the labels and store it in the array. We don't use np.concatenate or append to save computation time\n",
    "for i in range(num_training_examples):\n",
    "    y_generated[i*64:(i+1)*64] = np.load(\"C:/Users/usuario/RecogniChess/Data Generation/Pre Processed Data Generated/Labels/EX_%04d.npy.npz\"  % i, allow_pickle=True)['arr_0'] # Load the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_generated.shape) \n",
    "print(y_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from balanced_loss import Loss\n",
    "\n",
    "# outputs and labels\n",
    "\n",
    "num_samples = len(y_generated)\n",
    "\n",
    "# percentage_per_class = [73.75, 7.19, 1.41, 1.56, 1.56, 0.78, 1.56, 6.41, 1.25, 1.41, 1.09, 0.47, 1.56]\n",
    "# samples_per_class = [int(num_samples * percentage / 100) for percentage in percentage_per_class]\n",
    "\n",
    "samples_per_class = [3693, 358, 8, 9, 9, 4, 9, 400, 7, 8, 6, 2, 9]\n",
    "print((samples_per_class))\n",
    "\n",
    "# class-balanced focal loss\n",
    "fLoss = Loss(\n",
    "    loss_type = \"focal_loss\",\n",
    "    beta = 2,\n",
    "    fl_gamma = 3,\n",
    "    samples_per_class = samples_per_class,\n",
    "    class_balanced = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_softmax = vgg16_softmax.to(\"cpu\")\n",
    "num_epochs = 30\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(vgg16_softmax.parameters(), lr=learning_rate)\n",
    "\n",
    "# Put the model in training mode\n",
    "vgg16_softmax.train()\n",
    "\n",
    "# Loop over all instances of EX_0000\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    # Loop over all instances of EX_0000\n",
    "    for i, (image, y_true) in enumerate(train_loader):\n",
    "        # Move the data to the GPU (if available)\n",
    "        image = image.to(\"cpu\")\n",
    "        y_true = y_true.to(\"cpu\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        output = vgg16_softmax(image) # Pass the input through the model\n",
    "        loss = fLoss(output, y_true) # Compute the loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss = loss.item()\n",
    "\n",
    "        _, y_pred = torch.max(output, 1) # Get the predicted class label   \n",
    "\n",
    "    print(f\"Epoch: {epoch}, Focal loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa1580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy, f1_score\n",
    "\n",
    "# Set the model to training mode\n",
    "vgg16_softmax.train()\n",
    "\n",
    "train_epoch = 0\n",
    "val_epoch = 0\n",
    "\n",
    "n = 50 # Compute validation loss every n iterations\n",
    "\n",
    "train_acc = 0\n",
    "#train_balanced_acc = 0\n",
    "train_F1 = 0\n",
    "loss = 0\n",
    "\n",
    "# Loop over all instances in train_data\n",
    "for i in range(len(train_data)):\n",
    "    # Load the input image\n",
    "    x = train_data[i][\"Image Vector\"]\n",
    "\n",
    "    # Convert the input image to a numpy array and apply the transform\n",
    "    example1 = np.array(x).astype(np.float32) / 255.0\n",
    "    example1 = (example1 * 255).astype(np.uint8)\n",
    "    example1 = transform(example1)\n",
    "    example1 = example1.unsqueeze(0)\n",
    "    example1 = example1[:, :3, :, :]\n",
    "\n",
    "    # Move the data to the GPU (if available)\n",
    "    example1 = example1.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    output = vgg16_softmax(example1) # Pass the input through the model\n",
    "\n",
    "    # Convert the true label to a tensor and move it to the GPU (if available)\n",
    "    y_true_t = torch.tensor([train_data[i][\"Piece Label\"]]).to(device)\n",
    "\n",
    "    loss = fLoss_train(output, y_true_t) # Compute the loss\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, y_pred_t = torch.max(output, 1) # Get the predicted class label\n",
    "    \n",
    "    # Compute the balanced accuracy\n",
    "    accuracy_t = accuracy(preds = y_pred_t, target = y_true_t, task=\"multiclass\", num_classes=13, average= 'weighted')\n",
    "    train_acc = accuracy_t / len(train_data) \n",
    "    F1_t = f1_score(preds = y_pred_t, target = y_true_t, task=\"multiclass\", num_classes=13, average= 'weighted')\n",
    "    train_F1 = F1_t\n",
    "    \n",
    "    if train_epoch % 25 == 24:\n",
    "        print(f\"Train Iteration: {i+1}, Predicted class: {y_pred_t.item()}, True class: {y_true_t.item()}, Train Loss:  {loss:.4f}, Balanced Train Accuracy: {train_acc:.2%}, , Weighted F1: {train_F1:.2}\")\n",
    "    \n",
    "    # Compute validation loss and accuracy every n iterations\n",
    "    if i % n == n-1:\n",
    "        # Set the model to evaluation mode\n",
    "        vgg16_softmax.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        # val_balanced_acc = 0\n",
    "        val_F1 = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for j in range(len(val_data)):\n",
    "                # Load the input image\n",
    "                x = val_data[j][\"Image Vector\"]\n",
    "\n",
    "                # Convert the input image to a numpy array and apply the transform\n",
    "                example2 = np.array(x).astype(np.float32) / 255.0\n",
    "                example2 = (example2 * 255).astype(np.uint8)\n",
    "                example2 = transform(example2)\n",
    "                example2 = example2.unsqueeze(0)\n",
    "                example2 = example2[:, :3, :, :]\n",
    "\n",
    "                # Move the data to the GPU (if available)\n",
    "                example2 = example2.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = vgg16_softmax(example2) # Pass the input through the model\n",
    "\n",
    "                # Convert the true label to a tensor and move it to the GPU (if available)\n",
    "                y_true_v = torch.tensor([val_data[j][\"Piece Label\"]]).to(device)\n",
    "\n",
    "                # Compute the loss and accuracy\n",
    "                val_loss = fLoss_val(output, y_true_v) # Compute the loss\n",
    "                _, y_pred_v = torch.max(output, 1) # Get the predicted class label   \n",
    "                \n",
    "                accuracy_v = accuracy(preds = y_pred_v, target = y_true_v, task=\"multiclass\", num_classes=13, average= 'weighted')               \n",
    "                val_acc = accuracy_v / len(val_data) # Compute the accuracy\n",
    "                F1_v = f1_score(preds = y_pred_v, target = y_true_v, task=\"multiclass\", num_classes=13, average= 'weighted')\n",
    "                val_F1 = F1_v         \n",
    "                \n",
    "                # Compute the balanced accuracy\n",
    "                y_pred_np = y_pred_v.cpu().numpy()\n",
    "                y_true_np = y_true_v.cpu().numpy()\n",
    "                # val_balanced_acc += balanced_accuracy_score(y_true_np, y_pred_np) / len(val_data)\n",
    "\n",
    "        # Print validation loss, accuracy, and balanced accuracy\n",
    "        val_epoch += 1\n",
    "        print(f\"Val Epoch: {val_epoch}, Scaled Val Loss:  {val_loss/len(val_data):.4f}, Balanced Val Acc: {val_acc:.2%}, Weighted F1: {val_F1:.2}\")\n",
    "\n",
    "        # Set the model back to training mode\n",
    "        vgg16_softmax.train()\n",
    "    \n",
    "    train_epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
