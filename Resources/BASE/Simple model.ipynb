{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b950cda7",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "144fa7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now load the dependencies\n",
    "%matplotlib inline \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision\n",
    "import torchsummary\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import sklearn.metrics as metrics\n",
    "import torchmetrics\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6211a13b",
   "metadata": {},
   "source": [
    "We can start by setting a seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "356ddda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b819c641d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8c3d84a",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6afe02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, label_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.images = os.listdir(root_dir)\n",
    "        self.labels = torch.load(label_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = transforms.ToTensor()(image)\n",
    "        label = self.labels[int(img_name[3:9])]\n",
    "        return image, label\n",
    "\n",
    "generated_data_root = \"../../Data Generation/Pre Processed Data Generated\"\n",
    "train_gen_dataset = CustomDataset(generated_data_root + \"/Square Images/Training\", generated_data_root + \"/Square Images/y_piece_generated.pt\")\n",
    "val_gen_dataset = CustomDataset(generated_data_root + \"/Square Images/Validation\", generated_data_root + \"/Square Images/y_piece_generated.pt\")\n",
    "test_gen_dataset = CustomDataset(generated_data_root + \"/Square Images/Testing\", generated_data_root + \"/Square Images/y_piece_generated.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b93703ce",
   "metadata": {},
   "source": [
    "# Hyperparameter choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b259e4e1",
   "metadata": {},
   "source": [
    "We create a cell to hold the hyperparameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6ce00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "num_epochs = 50\n",
    "gamma_focal_loss_choices = {2, 3}\n",
    "dropout_rate_choices = {0.2, 0.5}\n",
    "n_loss = 100\n",
    "n_eval_minibatches = 200 # Number of minibatches to use for validation every epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "385914aa",
   "metadata": {},
   "source": [
    "We can now create our dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cec326c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train_loader = DataLoader(train_gen_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "gen_val_loader = DataLoader(val_gen_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "gen_test_loader = DataLoader(test_gen_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# real_train_loader = torch.load('../../Real life data/Pre-processed/partial_real_train_loader')\n",
    "\n",
    "# real_val_loader = torch.load('../../Real life data/Pre-processed/partial_real_val_loader')\n",
    "\n",
    "# real_test_loader = torch.load('../../Real life data/Pre-processed/partial_real_test_loader')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e60fc99",
   "metadata": {},
   "source": [
    "# Model implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "868ef8b6",
   "metadata": {},
   "source": [
    "We can start by loading a pre-trained VGG16 model without the classification layers towards the end (Only the feature extractor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a976a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8433f526",
   "metadata": {},
   "source": [
    "We can now visualize its layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9614fd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "018e9568",
   "metadata": {},
   "source": [
    "Because we are looking for a pre-trained feature extractor here, we decide to only use the features part and freeze its weights. We can then add a few subsequent layers to fine tune predictions. We can thus define the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a051bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=13, dropout_rate=0.5):\n",
    "        \n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        # Define the layers of the model\n",
    "        self.features = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4608, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Set the features to not require gradients\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7348424",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3c49179",
   "metadata": {},
   "source": [
    "We can start by finding the device to use for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1237640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61cb4352",
   "metadata": {},
   "source": [
    "We can then go ahead and define the loss function we will be using. Because we will opt for a balanced focal loss instead of a regular cross entropy loss which gives more importance to the classes that are harder to classify. We thus implement the focal loss defined by the following formula:\n",
    "<center><img src=\"focal loss.png\"></center>\n",
    "\n",
    "where gamma is a tunable hyperparameter. We also further add an alpha term to handle class imbalance, making our loss function a class-balanced focal loss, as shown in https://github.com/AdeelH/pytorch-multi-class-focal-loss.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "057bb473",
   "metadata": {},
   "source": [
    "Finally, we need an accuracy metric to tune the hyperparameters of the model. We will opt for a balanced accuracy score, which is just regular classification accuracy but adapted to weigh each class by its frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3ba9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=13, average=\"weighted\").to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2047f74c",
   "metadata": {},
   "source": [
    "Finally, because we are using balanced accuracy scores, we can use the class analytics gathered during pre-processing to define the following class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36161b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_proportions = [0.3198, 0.1602, 0.0405, 0.0400, 0.0406, 0.0201, 0.0404, 0.1596, 0.0392, 0.0397, 0.0400, 0.0196, 0.0404]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e0fb873",
   "metadata": {},
   "source": [
    "We can now proceed to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02e90133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Iteration [1], Training Loss: [2.5568]\n",
      "Epoch [1], Iteration [2], Training Loss: [2.2777]\n",
      "Epoch [1], Iteration [3], Training Loss: [1.8181]\n",
      "Epoch [1], Iteration [4], Training Loss: [1.3611]\n",
      "Epoch [1], Iteration [5], Training Loss: [1.9429]\n",
      "Epoch [1], Iteration [6], Training Loss: [1.5171]\n",
      "Epoch [1], Iteration [7], Training Loss: [1.2792]\n",
      "Epoch [1], Iteration [8], Training Loss: [0.8377]\n",
      "Epoch [1], Iteration [9], Training Loss: [1.0715]\n",
      "Epoch [1], Iteration [10], Training Loss: [0.8796]\n",
      "Epoch [1], Iteration [11], Training Loss: [0.8335]\n",
      "Epoch [1], Iteration [12], Training Loss: [0.8278]\n",
      "Epoch [1], Iteration [13], Training Loss: [0.7963]\n",
      "Epoch [1], Iteration [14], Training Loss: [0.9164]\n",
      "Epoch [1], Iteration [15], Training Loss: [0.3773]\n",
      "Epoch [1], Iteration [16], Training Loss: [0.8018]\n",
      "Epoch [1], Iteration [17], Training Loss: [0.6292]\n",
      "Epoch [1], Iteration [18], Training Loss: [0.6726]\n",
      "Epoch [1], Iteration [19], Training Loss: [0.5733]\n",
      "Epoch [1], Iteration [20], Training Loss: [0.4696]\n",
      "Epoch [1], Iteration [21], Training Loss: [0.6587]\n",
      "Epoch [1], Iteration [22], Training Loss: [0.3531]\n",
      "Epoch [1], Iteration [23], Training Loss: [0.8151]\n",
      "Epoch [1], Iteration [24], Training Loss: [0.4903]\n",
      "Epoch [1], Iteration [25], Training Loss: [0.4353]\n",
      "Epoch [1], Iteration [26], Training Loss: [0.4411]\n",
      "Epoch [1], Iteration [27], Training Loss: [0.5157]\n",
      "Epoch [1], Iteration [28], Training Loss: [0.5120]\n",
      "Epoch [1], Iteration [29], Training Loss: [0.4253]\n",
      "Epoch [1], Iteration [30], Training Loss: [0.4631]\n",
      "Epoch [1], Iteration [31], Training Loss: [0.3345]\n",
      "Epoch [1], Iteration [32], Training Loss: [0.3433]\n",
      "Epoch [1], Iteration [33], Training Loss: [0.3653]\n",
      "Epoch [1], Iteration [34], Training Loss: [0.5326]\n",
      "Epoch [1], Iteration [35], Training Loss: [0.2402]\n",
      "Epoch [1], Iteration [36], Training Loss: [0.5504]\n",
      "Epoch [1], Iteration [37], Training Loss: [0.6054]\n",
      "Epoch [1], Iteration [38], Training Loss: [0.3333]\n",
      "Epoch [1], Iteration [39], Training Loss: [0.3847]\n",
      "Epoch [1], Iteration [40], Training Loss: [0.2162]\n",
      "Epoch [1], Iteration [41], Training Loss: [0.2322]\n",
      "Epoch [1], Iteration [42], Training Loss: [0.3596]\n",
      "Epoch [1], Iteration [43], Training Loss: [0.3675]\n",
      "Epoch [1], Iteration [44], Training Loss: [0.3862]\n",
      "Epoch [1], Iteration [45], Training Loss: [0.6010]\n",
      "Epoch [1], Iteration [46], Training Loss: [0.2578]\n",
      "Epoch [1], Iteration [47], Training Loss: [0.3097]\n",
      "Epoch [1], Iteration [48], Training Loss: [0.3990]\n",
      "Epoch [1], Iteration [49], Training Loss: [0.3130]\n",
      "Epoch [1], Iteration [50], Training Loss: [0.4072]\n",
      "Epoch [1], Iteration [51], Training Loss: [0.3189]\n",
      "Epoch [1], Iteration [52], Training Loss: [0.3569]\n",
      "Epoch [1], Iteration [53], Training Loss: [0.2235]\n",
      "Epoch [1], Iteration [54], Training Loss: [0.2882]\n",
      "Epoch [1], Iteration [55], Training Loss: [0.1257]\n",
      "Epoch [1], Iteration [56], Training Loss: [0.6592]\n",
      "Epoch [1], Iteration [57], Training Loss: [0.3340]\n",
      "Epoch [1], Iteration [58], Training Loss: [0.2245]\n",
      "Epoch [1], Iteration [59], Training Loss: [0.2582]\n",
      "Epoch [1], Iteration [60], Training Loss: [0.2887]\n",
      "Epoch [1], Iteration [61], Training Loss: [0.2964]\n",
      "Epoch [1], Iteration [62], Training Loss: [0.2314]\n",
      "Epoch [1], Iteration [63], Training Loss: [0.1620]\n",
      "Epoch [1], Iteration [64], Training Loss: [0.1436]\n",
      "Epoch [1], Iteration [65], Training Loss: [0.4180]\n",
      "Epoch [1], Iteration [66], Training Loss: [0.2390]\n",
      "Epoch [1], Iteration [67], Training Loss: [0.1544]\n",
      "Epoch [1], Iteration [68], Training Loss: [0.2243]\n",
      "Epoch [1], Iteration [69], Training Loss: [0.1935]\n",
      "Epoch [1], Iteration [70], Training Loss: [0.1700]\n",
      "Epoch [1], Iteration [71], Training Loss: [0.1510]\n",
      "Epoch [1], Iteration [72], Training Loss: [0.1717]\n",
      "Epoch [1], Iteration [73], Training Loss: [0.1666]\n",
      "Epoch [1], Iteration [74], Training Loss: [0.1522]\n",
      "Epoch [1], Iteration [75], Training Loss: [0.2172]\n",
      "Epoch [1], Iteration [76], Training Loss: [0.1800]\n",
      "Epoch [1], Iteration [77], Training Loss: [0.1712]\n",
      "Epoch [1], Iteration [78], Training Loss: [0.1147]\n",
      "Epoch [1], Iteration [79], Training Loss: [0.1188]\n",
      "Epoch [1], Iteration [80], Training Loss: [0.0867]\n",
      "Epoch [1], Iteration [81], Training Loss: [0.0978]\n",
      "Epoch [1], Iteration [82], Training Loss: [0.0681]\n",
      "Epoch [1], Iteration [83], Training Loss: [0.2068]\n",
      "Epoch [1], Iteration [84], Training Loss: [0.1211]\n",
      "Epoch [1], Iteration [85], Training Loss: [0.2758]\n",
      "Epoch [1], Iteration [86], Training Loss: [0.1544]\n",
      "Epoch [1], Iteration [87], Training Loss: [0.1911]\n",
      "Epoch [1], Iteration [88], Training Loss: [0.1062]\n",
      "Epoch [1], Iteration [89], Training Loss: [0.2266]\n",
      "Epoch [1], Iteration [90], Training Loss: [0.1834]\n",
      "Epoch [1], Iteration [91], Training Loss: [0.2009]\n",
      "Epoch [1], Iteration [92], Training Loss: [0.3415]\n",
      "Epoch [1], Iteration [93], Training Loss: [0.2116]\n",
      "Epoch [1], Iteration [94], Training Loss: [0.1496]\n",
      "Epoch [1], Iteration [95], Training Loss: [0.2415]\n",
      "Epoch [1], Iteration [96], Training Loss: [0.2319]\n",
      "Epoch [1], Iteration [97], Training Loss: [0.1273]\n",
      "Epoch [1], Iteration [98], Training Loss: [0.2251]\n",
      "Epoch [1], Iteration [99], Training Loss: [0.1960]\n",
      "Epoch [1], Iteration [100], Training Loss: [0.1072]\n",
      "Epoch [1], Iteration [101], Training Loss: [0.2406]\n",
      "Epoch [1], Iteration [102], Training Loss: [0.1878]\n",
      "Epoch [1], Iteration [103], Training Loss: [0.1651]\n",
      "Epoch [1], Iteration [104], Training Loss: [0.1247]\n",
      "Epoch [1], Iteration [105], Training Loss: [0.1484]\n",
      "Epoch [1], Iteration [106], Training Loss: [0.1858]\n",
      "Epoch [1], Iteration [107], Training Loss: [0.3466]\n",
      "Epoch [1], Iteration [108], Training Loss: [0.2077]\n",
      "Epoch [1], Iteration [109], Training Loss: [0.2830]\n",
      "Epoch [1], Iteration [110], Training Loss: [0.1223]\n",
      "Epoch [1], Iteration [111], Training Loss: [0.2910]\n",
      "Epoch [1], Iteration [112], Training Loss: [0.1447]\n",
      "Epoch [1], Iteration [113], Training Loss: [0.2464]\n",
      "Epoch [1], Iteration [114], Training Loss: [0.1412]\n",
      "Epoch [1], Iteration [115], Training Loss: [0.1622]\n",
      "Epoch [1], Iteration [116], Training Loss: [0.2501]\n",
      "Epoch [1], Iteration [117], Training Loss: [0.3814]\n",
      "Epoch [1], Iteration [118], Training Loss: [0.2229]\n",
      "Epoch [1], Iteration [119], Training Loss: [0.1439]\n",
      "Epoch [1], Iteration [120], Training Loss: [0.2174]\n",
      "Epoch [1], Iteration [121], Training Loss: [0.2125]\n",
      "Epoch [1], Iteration [122], Training Loss: [0.2080]\n",
      "Epoch [1], Iteration [123], Training Loss: [0.1336]\n",
      "Epoch [1], Iteration [124], Training Loss: [0.1017]\n",
      "Epoch [1], Iteration [125], Training Loss: [0.0888]\n",
      "Epoch [1], Iteration [126], Training Loss: [0.0867]\n",
      "Epoch [1], Iteration [127], Training Loss: [0.0873]\n",
      "Epoch [1], Iteration [128], Training Loss: [0.0898]\n",
      "Epoch [1], Iteration [129], Training Loss: [0.2493]\n",
      "Epoch [1], Iteration [130], Training Loss: [0.1307]\n",
      "Epoch [1], Iteration [131], Training Loss: [0.1350]\n",
      "Epoch [1], Iteration [132], Training Loss: [0.1293]\n",
      "Epoch [1], Iteration [133], Training Loss: [0.1181]\n",
      "Epoch [1], Iteration [134], Training Loss: [0.1087]\n",
      "Epoch [1], Iteration [135], Training Loss: [0.0573]\n",
      "Epoch [1], Iteration [136], Training Loss: [0.1145]\n",
      "Epoch [1], Iteration [137], Training Loss: [0.0126]\n",
      "Epoch [1], Iteration [138], Training Loss: [0.0873]\n",
      "Epoch [1], Iteration [139], Training Loss: [0.0953]\n",
      "Epoch [1], Iteration [140], Training Loss: [0.1035]\n",
      "Epoch [1], Iteration [141], Training Loss: [0.0662]\n",
      "Epoch [1], Iteration [142], Training Loss: [0.1281]\n",
      "Epoch [1], Iteration [143], Training Loss: [0.0599]\n",
      "Epoch [1], Iteration [144], Training Loss: [0.0870]\n",
      "Epoch [1], Iteration [145], Training Loss: [0.1169]\n",
      "Epoch [1], Iteration [146], Training Loss: [0.1409]\n",
      "Epoch [1], Iteration [147], Training Loss: [0.1506]\n",
      "Epoch [1], Iteration [148], Training Loss: [0.1264]\n",
      "Epoch [1], Iteration [149], Training Loss: [0.0464]\n",
      "Epoch [1], Iteration [150], Training Loss: [0.0289]\n",
      "Epoch [1], Iteration [151], Training Loss: [0.0871]\n",
      "Epoch [1], Iteration [152], Training Loss: [0.0829]\n",
      "Epoch [1], Iteration [153], Training Loss: [0.0675]\n",
      "Epoch [1], Iteration [154], Training Loss: [0.2861]\n",
      "Epoch [1], Iteration [155], Training Loss: [0.0481]\n",
      "Epoch [1], Iteration [156], Training Loss: [0.1753]\n",
      "Epoch [1], Iteration [157], Training Loss: [0.0337]\n",
      "Epoch [1], Iteration [158], Training Loss: [0.0451]\n",
      "Epoch [1], Iteration [159], Training Loss: [0.0978]\n",
      "Epoch [1], Iteration [160], Training Loss: [0.0576]\n",
      "Epoch [1], Iteration [161], Training Loss: [0.0527]\n",
      "Epoch [1], Iteration [162], Training Loss: [0.0882]\n",
      "Epoch [1], Iteration [163], Training Loss: [0.0832]\n",
      "Epoch [1], Iteration [164], Training Loss: [0.0550]\n",
      "Epoch [1], Iteration [165], Training Loss: [0.1024]\n",
      "Epoch [1], Iteration [166], Training Loss: [0.0766]\n",
      "Epoch [1], Iteration [167], Training Loss: [0.0481]\n",
      "Epoch [1], Iteration [168], Training Loss: [0.0892]\n",
      "Epoch [1], Iteration [169], Training Loss: [0.2846]\n",
      "Epoch [1], Iteration [170], Training Loss: [0.1472]\n",
      "Epoch [1], Iteration [171], Training Loss: [0.0273]\n",
      "Epoch [1], Iteration [172], Training Loss: [0.2053]\n",
      "Epoch [1], Iteration [173], Training Loss: [0.0537]\n",
      "Epoch [1], Iteration [174], Training Loss: [0.1623]\n",
      "Epoch [1], Iteration [175], Training Loss: [0.1816]\n",
      "Epoch [1], Iteration [176], Training Loss: [0.0837]\n",
      "Epoch [1], Iteration [177], Training Loss: [0.0950]\n",
      "Epoch [1], Iteration [178], Training Loss: [0.0581]\n",
      "Epoch [1], Iteration [179], Training Loss: [0.1099]\n",
      "Epoch [1], Iteration [180], Training Loss: [0.1128]\n",
      "Epoch [1], Iteration [181], Training Loss: [0.0884]\n",
      "Epoch [1], Iteration [182], Training Loss: [0.1445]\n",
      "Epoch [1], Iteration [183], Training Loss: [0.0308]\n",
      "Epoch [1], Iteration [184], Training Loss: [0.0902]\n",
      "Epoch [1], Iteration [185], Training Loss: [0.0429]\n",
      "Epoch [1], Iteration [186], Training Loss: [0.1318]\n",
      "Epoch [1], Iteration [187], Training Loss: [0.3935]\n",
      "Epoch [1], Iteration [188], Training Loss: [0.1087]\n",
      "Epoch [1], Iteration [189], Training Loss: [0.1206]\n",
      "Epoch [1], Iteration [190], Training Loss: [0.0792]\n",
      "Epoch [1], Iteration [191], Training Loss: [0.0969]\n",
      "Epoch [1], Iteration [192], Training Loss: [0.0488]\n",
      "Epoch [1], Iteration [193], Training Loss: [0.3178]\n",
      "Epoch [1], Iteration [194], Training Loss: [0.0772]\n",
      "Epoch [1], Iteration [195], Training Loss: [0.2104]\n",
      "Epoch [1], Iteration [196], Training Loss: [0.1126]\n",
      "Epoch [1], Iteration [197], Training Loss: [0.2561]\n",
      "Epoch [1], Iteration [198], Training Loss: [0.0994]\n",
      "Epoch [1], Iteration [199], Training Loss: [0.0481]\n",
      "Epoch [1], Iteration [200], Training Loss: [0.1707]\n",
      "Epoch [1], Iteration [201], Training Loss: [0.1526]\n",
      "Epoch [1], Iteration [202], Training Loss: [0.1624]\n",
      "Epoch [1], Iteration [203], Training Loss: [0.0358]\n",
      "Epoch [1], Iteration [204], Training Loss: [0.0401]\n",
      "Epoch [1], Iteration [205], Training Loss: [0.0858]\n",
      "Epoch [1], Iteration [206], Training Loss: [0.1072]\n",
      "Epoch [1], Iteration [207], Training Loss: [0.0613]\n",
      "Epoch [1], Iteration [208], Training Loss: [0.0285]\n",
      "Epoch [1], Iteration [209], Training Loss: [0.0397]\n",
      "Epoch [1], Iteration [210], Training Loss: [0.1580]\n",
      "Epoch [1], Iteration [211], Training Loss: [0.0844]\n",
      "Epoch [1], Iteration [212], Training Loss: [0.1381]\n",
      "Epoch [1], Iteration [213], Training Loss: [0.0728]\n",
      "Epoch [1], Iteration [214], Training Loss: [0.0465]\n",
      "Epoch [1], Iteration [215], Training Loss: [0.1302]\n",
      "Epoch [1], Iteration [216], Training Loss: [0.0803]\n",
      "Epoch [1], Iteration [217], Training Loss: [0.0758]\n",
      "Epoch [1], Iteration [218], Training Loss: [0.0690]\n",
      "Epoch [1], Iteration [219], Training Loss: [0.0831]\n",
      "Epoch [1], Iteration [220], Training Loss: [0.1584]\n",
      "Epoch [1], Iteration [221], Training Loss: [0.1449]\n",
      "Epoch [1], Iteration [222], Training Loss: [0.1557]\n",
      "Epoch [1], Iteration [223], Training Loss: [0.0132]\n",
      "Epoch [1], Iteration [224], Training Loss: [0.0412]\n",
      "Epoch [1], Iteration [225], Training Loss: [0.1332]\n",
      "Epoch [1], Iteration [226], Training Loss: [0.0855]\n",
      "Epoch [1], Iteration [227], Training Loss: [0.1017]\n",
      "Epoch [1], Iteration [228], Training Loss: [0.0773]\n",
      "Epoch [1], Iteration [229], Training Loss: [0.2780]\n",
      "Epoch [1], Iteration [230], Training Loss: [0.1632]\n",
      "Epoch [1], Iteration [231], Training Loss: [0.1036]\n",
      "Epoch [1], Iteration [232], Training Loss: [0.0467]\n",
      "Epoch [1], Iteration [233], Training Loss: [0.0768]\n",
      "Epoch [1], Iteration [234], Training Loss: [0.1145]\n",
      "Epoch [1], Iteration [235], Training Loss: [0.0712]\n",
      "Epoch [1], Iteration [236], Training Loss: [0.1030]\n",
      "Epoch [1], Iteration [237], Training Loss: [0.0831]\n",
      "Epoch [1], Iteration [238], Training Loss: [0.3825]\n",
      "Epoch [1], Iteration [239], Training Loss: [0.0643]\n",
      "Epoch [1], Iteration [240], Training Loss: [0.0545]\n",
      "Epoch [1], Iteration [241], Training Loss: [0.0720]\n",
      "Epoch [1], Iteration [242], Training Loss: [0.0459]\n",
      "Epoch [1], Iteration [243], Training Loss: [0.0317]\n",
      "Epoch [1], Iteration [244], Training Loss: [0.0913]\n",
      "Epoch [1], Iteration [245], Training Loss: [0.1944]\n",
      "Epoch [1], Iteration [246], Training Loss: [0.0846]\n",
      "Epoch [1], Iteration [247], Training Loss: [0.1418]\n",
      "Epoch [1], Iteration [248], Training Loss: [0.1120]\n",
      "Epoch [1], Iteration [249], Training Loss: [0.1267]\n",
      "Epoch [1], Iteration [250], Training Loss: [0.1223]\n",
      "Epoch [1], Iteration [251], Training Loss: [0.1671]\n",
      "Epoch [1], Iteration [252], Training Loss: [0.0167]\n",
      "Epoch [1], Iteration [253], Training Loss: [0.0482]\n",
      "Epoch [1], Iteration [254], Training Loss: [0.0322]\n",
      "Epoch [1], Iteration [255], Training Loss: [0.0583]\n",
      "Epoch [1], Iteration [256], Training Loss: [0.0476]\n",
      "Epoch [1], Iteration [257], Training Loss: [0.0413]\n",
      "Epoch [1], Iteration [258], Training Loss: [0.3102]\n",
      "Epoch [1], Iteration [259], Training Loss: [0.0252]\n",
      "Epoch [1], Iteration [260], Training Loss: [0.1116]\n",
      "Epoch [1], Iteration [261], Training Loss: [0.0860]\n",
      "Epoch [1], Iteration [262], Training Loss: [0.0318]\n",
      "Epoch [1], Iteration [263], Training Loss: [0.0550]\n",
      "Epoch [1], Iteration [264], Training Loss: [0.0144]\n",
      "Epoch [1], Iteration [265], Training Loss: [0.0680]\n",
      "Epoch [1], Iteration [266], Training Loss: [0.1672]\n",
      "Epoch [1], Iteration [267], Training Loss: [0.1859]\n",
      "Epoch [1], Iteration [268], Training Loss: [0.0193]\n",
      "Epoch [1], Iteration [269], Training Loss: [0.0687]\n",
      "Epoch [1], Iteration [270], Training Loss: [0.0726]\n",
      "Epoch [1], Iteration [271], Training Loss: [0.0121]\n",
      "Epoch [1], Iteration [272], Training Loss: [0.0961]\n",
      "Epoch [1], Iteration [273], Training Loss: [0.0858]\n",
      "Epoch [1], Iteration [274], Training Loss: [0.2955]\n",
      "Epoch [1], Iteration [275], Training Loss: [0.0912]\n",
      "Epoch [1], Iteration [276], Training Loss: [0.1124]\n",
      "Epoch [1], Iteration [277], Training Loss: [0.0496]\n",
      "Epoch [1], Iteration [278], Training Loss: [0.0144]\n",
      "Epoch [1], Iteration [279], Training Loss: [0.1254]\n",
      "Epoch [1], Iteration [280], Training Loss: [0.1575]\n",
      "Epoch [1], Iteration [281], Training Loss: [0.1707]\n",
      "Epoch [1], Iteration [282], Training Loss: [0.0692]\n",
      "Epoch [1], Iteration [283], Training Loss: [0.0501]\n",
      "Epoch [1], Iteration [284], Training Loss: [0.0795]\n",
      "Epoch [1], Iteration [285], Training Loss: [0.0579]\n",
      "Epoch [1], Iteration [286], Training Loss: [0.1147]\n",
      "Epoch [1], Iteration [287], Training Loss: [0.0966]\n",
      "Epoch [1], Iteration [288], Training Loss: [0.1095]\n",
      "Epoch [1], Iteration [289], Training Loss: [0.0668]\n",
      "Epoch [1], Iteration [290], Training Loss: [0.0354]\n",
      "Epoch [1], Iteration [291], Training Loss: [0.0356]\n",
      "Epoch [1], Iteration [292], Training Loss: [0.0757]\n",
      "Epoch [1], Iteration [293], Training Loss: [0.1155]\n",
      "Epoch [1], Iteration [294], Training Loss: [0.1938]\n",
      "Epoch [1], Iteration [295], Training Loss: [0.0480]\n",
      "Epoch [1], Iteration [296], Training Loss: [0.0725]\n",
      "Epoch [1], Iteration [297], Training Loss: [0.0561]\n",
      "Epoch [1], Iteration [298], Training Loss: [0.0160]\n",
      "Epoch [1], Iteration [299], Training Loss: [0.0383]\n",
      "Epoch [1], Iteration [300], Training Loss: [0.0246]\n",
      "Epoch [1], Iteration [301], Training Loss: [0.0591]\n",
      "Epoch [1], Iteration [302], Training Loss: [0.1226]\n",
      "Epoch [1], Iteration [303], Training Loss: [0.0544]\n",
      "Epoch [1], Iteration [304], Training Loss: [0.0402]\n",
      "Epoch [1], Iteration [305], Training Loss: [0.0651]\n",
      "Epoch [1], Iteration [306], Training Loss: [0.1337]\n",
      "Epoch [1], Iteration [307], Training Loss: [0.1058]\n",
      "Epoch [1], Iteration [308], Training Loss: [0.0208]\n",
      "Epoch [1], Iteration [309], Training Loss: [0.0717]\n",
      "Epoch [1], Iteration [310], Training Loss: [0.0073]\n",
      "Epoch [1], Iteration [311], Training Loss: [0.0089]\n",
      "Epoch [1], Iteration [312], Training Loss: [0.1063]\n",
      "Epoch [1], Iteration [313], Training Loss: [0.0733]\n",
      "Epoch [1], Iteration [314], Training Loss: [0.0441]\n",
      "Epoch [1], Iteration [315], Training Loss: [0.0788]\n",
      "Epoch [1], Iteration [316], Training Loss: [0.0044]\n",
      "Epoch [1], Iteration [317], Training Loss: [0.0463]\n",
      "Epoch [1], Iteration [318], Training Loss: [0.1053]\n",
      "Epoch [1], Iteration [319], Training Loss: [0.0834]\n",
      "Epoch [1], Iteration [320], Training Loss: [0.0203]\n",
      "Epoch [1], Iteration [321], Training Loss: [0.0165]\n",
      "Epoch [1], Iteration [322], Training Loss: [0.0317]\n",
      "Epoch [1], Iteration [323], Training Loss: [0.0843]\n",
      "Epoch [1], Iteration [324], Training Loss: [0.0113]\n",
      "Epoch [1], Iteration [325], Training Loss: [0.0147]\n",
      "Epoch [1], Iteration [326], Training Loss: [0.1818]\n",
      "Epoch [1], Iteration [327], Training Loss: [0.0749]\n",
      "Epoch [1], Iteration [328], Training Loss: [0.0175]\n",
      "Epoch [1], Iteration [329], Training Loss: [0.0595]\n",
      "Epoch [1], Iteration [330], Training Loss: [0.0396]\n",
      "Epoch [1], Iteration [331], Training Loss: [0.0423]\n",
      "Epoch [1], Iteration [332], Training Loss: [0.0345]\n",
      "Epoch [1], Iteration [333], Training Loss: [0.0883]\n",
      "Epoch [1], Iteration [334], Training Loss: [0.0503]\n",
      "Epoch [1], Iteration [335], Training Loss: [0.1656]\n",
      "Epoch [1], Iteration [336], Training Loss: [0.0389]\n",
      "Epoch [1], Iteration [337], Training Loss: [0.0140]\n",
      "Epoch [1], Iteration [338], Training Loss: [0.0848]\n",
      "Epoch [1], Iteration [339], Training Loss: [0.1554]\n",
      "Epoch [1], Iteration [340], Training Loss: [0.0410]\n",
      "Epoch [1], Iteration [341], Training Loss: [0.0591]\n",
      "Epoch [1], Iteration [342], Training Loss: [0.0440]\n",
      "Epoch [1], Iteration [343], Training Loss: [0.0255]\n",
      "Epoch [1], Iteration [344], Training Loss: [0.0642]\n",
      "Epoch [1], Iteration [345], Training Loss: [0.0085]\n",
      "Epoch [1], Iteration [346], Training Loss: [0.0148]\n",
      "Epoch [1], Iteration [347], Training Loss: [0.1029]\n",
      "Epoch [1], Iteration [348], Training Loss: [0.0181]\n",
      "Epoch [1], Iteration [349], Training Loss: [0.0732]\n",
      "Epoch [1], Iteration [350], Training Loss: [0.0970]\n",
      "Epoch [1], Iteration [351], Training Loss: [0.1669]\n",
      "Epoch [1], Iteration [352], Training Loss: [0.0238]\n",
      "Epoch [1], Iteration [353], Training Loss: [0.0601]\n",
      "Epoch [1], Iteration [354], Training Loss: [0.1252]\n",
      "Epoch [1], Iteration [355], Training Loss: [0.0493]\n",
      "Epoch [1], Iteration [356], Training Loss: [0.0140]\n",
      "Epoch [1], Iteration [357], Training Loss: [0.0109]\n",
      "Epoch [1], Iteration [358], Training Loss: [0.0678]\n",
      "Epoch [1], Iteration [359], Training Loss: [0.0142]\n",
      "Epoch [1], Iteration [360], Training Loss: [0.0452]\n",
      "Epoch [1], Iteration [361], Training Loss: [0.0240]\n",
      "Epoch [1], Iteration [362], Training Loss: [0.0881]\n",
      "Epoch [1], Iteration [363], Training Loss: [0.0475]\n",
      "Epoch [1], Iteration [364], Training Loss: [0.1714]\n",
      "Epoch [1], Iteration [365], Training Loss: [0.2168]\n",
      "Epoch [1], Iteration [366], Training Loss: [0.0054]\n",
      "Epoch [1], Iteration [367], Training Loss: [0.1256]\n",
      "Epoch [1], Iteration [368], Training Loss: [0.0733]\n",
      "Epoch [1], Iteration [369], Training Loss: [0.0502]\n",
      "Epoch [1], Iteration [370], Training Loss: [0.0624]\n",
      "Epoch [1], Iteration [371], Training Loss: [0.0382]\n",
      "Epoch [1], Iteration [372], Training Loss: [0.1150]\n",
      "Epoch [1], Iteration [373], Training Loss: [0.0069]\n",
      "Epoch [1], Iteration [374], Training Loss: [0.0981]\n",
      "Epoch [1], Iteration [375], Training Loss: [0.0166]\n",
      "Epoch [1], Iteration [376], Training Loss: [0.0802]\n",
      "Epoch [1], Iteration [377], Training Loss: [0.2215]\n",
      "Epoch [1], Iteration [378], Training Loss: [0.0616]\n",
      "Epoch [1], Iteration [379], Training Loss: [0.0059]\n",
      "Epoch [1], Iteration [380], Training Loss: [0.0077]\n",
      "Epoch [1], Iteration [381], Training Loss: [0.1279]\n",
      "Epoch [1], Iteration [382], Training Loss: [0.0157]\n",
      "Epoch [1], Iteration [383], Training Loss: [0.0284]\n",
      "Epoch [1], Iteration [384], Training Loss: [0.0811]\n",
      "Epoch [1], Iteration [385], Training Loss: [0.0520]\n",
      "Epoch [1], Iteration [386], Training Loss: [0.0507]\n",
      "Epoch [1], Iteration [387], Training Loss: [0.0259]\n",
      "Epoch [1], Iteration [388], Training Loss: [0.1735]\n",
      "Epoch [1], Iteration [389], Training Loss: [0.0245]\n",
      "Epoch [1], Iteration [390], Training Loss: [0.0333]\n",
      "Epoch [1], Iteration [391], Training Loss: [0.0171]\n",
      "Epoch [1], Iteration [392], Training Loss: [0.0785]\n",
      "Epoch [1], Iteration [393], Training Loss: [0.0340]\n",
      "Epoch [1], Iteration [394], Training Loss: [0.0130]\n",
      "Epoch [1], Iteration [395], Training Loss: [0.0218]\n",
      "Epoch [1], Iteration [396], Training Loss: [0.0439]\n",
      "Epoch [1], Iteration [397], Training Loss: [0.0476]\n",
      "Epoch [1], Iteration [398], Training Loss: [0.0130]\n",
      "Epoch [1], Iteration [399], Training Loss: [0.0137]\n",
      "Epoch [1], Iteration [400], Training Loss: [0.0705]\n",
      "Epoch [1], Iteration [401], Training Loss: [0.0680]\n",
      "Epoch [1], Iteration [402], Training Loss: [0.0376]\n",
      "Epoch [1], Iteration [403], Training Loss: [0.0902]\n",
      "Epoch [1], Iteration [404], Training Loss: [0.0635]\n",
      "Epoch [1], Iteration [405], Training Loss: [0.0363]\n",
      "Epoch [1], Iteration [406], Training Loss: [0.0039]\n",
      "Epoch [1], Iteration [407], Training Loss: [0.0150]\n",
      "Epoch [1], Iteration [408], Training Loss: [0.1764]\n",
      "Epoch [1], Iteration [409], Training Loss: [0.0625]\n",
      "Epoch [1], Iteration [410], Training Loss: [0.0845]\n",
      "Epoch [1], Iteration [411], Training Loss: [0.0340]\n",
      "Epoch [1], Iteration [412], Training Loss: [0.1974]\n",
      "Epoch [1], Iteration [413], Training Loss: [0.0178]\n",
      "Epoch [1], Iteration [414], Training Loss: [0.0336]\n",
      "Epoch [1], Iteration [415], Training Loss: [0.1751]\n",
      "Epoch [1], Iteration [416], Training Loss: [0.0358]\n",
      "Epoch [1], Iteration [417], Training Loss: [0.2526]\n",
      "Epoch [1], Iteration [418], Training Loss: [0.0162]\n",
      "Epoch [1], Iteration [419], Training Loss: [0.0450]\n",
      "Epoch [1], Iteration [420], Training Loss: [0.0283]\n",
      "Epoch [1], Iteration [421], Training Loss: [0.0331]\n",
      "Epoch [1], Iteration [422], Training Loss: [0.0623]\n",
      "Epoch [1], Iteration [423], Training Loss: [0.0598]\n",
      "Epoch [1], Iteration [424], Training Loss: [0.1448]\n",
      "Epoch [1], Iteration [425], Training Loss: [0.1057]\n",
      "Epoch [1], Iteration [426], Training Loss: [0.0857]\n",
      "Epoch [1], Iteration [427], Training Loss: [0.1267]\n",
      "Epoch [1], Iteration [428], Training Loss: [0.0369]\n",
      "Epoch [1], Iteration [429], Training Loss: [0.0561]\n",
      "Epoch [1], Iteration [430], Training Loss: [0.1151]\n",
      "Epoch [1], Iteration [431], Training Loss: [0.0219]\n",
      "Epoch [1], Iteration [432], Training Loss: [0.0632]\n",
      "Epoch [1], Iteration [433], Training Loss: [0.1021]\n",
      "Epoch [1], Iteration [434], Training Loss: [0.0049]\n",
      "Epoch [1], Iteration [435], Training Loss: [0.0753]\n",
      "Epoch [1], Iteration [436], Training Loss: [0.2271]\n",
      "Epoch [1], Iteration [437], Training Loss: [0.0602]\n",
      "Epoch [1], Iteration [438], Training Loss: [0.0408]\n",
      "Epoch [1], Iteration [439], Training Loss: [0.0893]\n",
      "Epoch [1], Iteration [440], Training Loss: [0.0089]\n",
      "Epoch [1], Iteration [441], Training Loss: [0.0543]\n",
      "Epoch [1], Iteration [442], Training Loss: [0.0117]\n",
      "Epoch [1], Iteration [443], Training Loss: [0.0077]\n",
      "Epoch [1], Iteration [444], Training Loss: [0.0228]\n",
      "Epoch [1], Iteration [445], Training Loss: [0.0078]\n",
      "Epoch [1], Iteration [446], Training Loss: [0.0250]\n",
      "Epoch [1], Iteration [447], Training Loss: [0.0359]\n",
      "Epoch [1], Iteration [448], Training Loss: [0.0245]\n",
      "Epoch [1], Iteration [449], Training Loss: [0.0395]\n",
      "Epoch [1], Iteration [450], Training Loss: [0.0643]\n",
      "Epoch [1], Iteration [451], Training Loss: [0.0716]\n",
      "Epoch [1], Iteration [452], Training Loss: [0.1533]\n",
      "Epoch [1], Iteration [453], Training Loss: [0.0048]\n",
      "Epoch [1], Iteration [454], Training Loss: [0.0113]\n",
      "Epoch [1], Iteration [455], Training Loss: [0.0400]\n",
      "Epoch [1], Iteration [456], Training Loss: [0.1953]\n",
      "Epoch [1], Iteration [457], Training Loss: [0.0435]\n",
      "Epoch [1], Iteration [458], Training Loss: [0.0420]\n",
      "Epoch [1], Iteration [459], Training Loss: [0.0316]\n",
      "Epoch [1], Iteration [460], Training Loss: [0.0213]\n",
      "Epoch [1], Iteration [461], Training Loss: [0.0503]\n",
      "Epoch [1], Iteration [462], Training Loss: [0.0196]\n",
      "Epoch [1], Iteration [463], Training Loss: [0.0195]\n",
      "Epoch [1], Iteration [464], Training Loss: [0.1288]\n",
      "Epoch [1], Iteration [465], Training Loss: [0.0766]\n",
      "Epoch [1], Iteration [466], Training Loss: [0.0240]\n",
      "Epoch [1], Iteration [467], Training Loss: [0.1050]\n",
      "Epoch [1], Iteration [468], Training Loss: [0.0102]\n",
      "Epoch [1], Iteration [469], Training Loss: [0.0195]\n",
      "Epoch [1], Iteration [470], Training Loss: [0.0486]\n",
      "Epoch [1], Iteration [471], Training Loss: [0.0278]\n",
      "Epoch [1], Iteration [472], Training Loss: [0.0168]\n",
      "Epoch [1], Iteration [473], Training Loss: [0.1102]\n",
      "Epoch [1], Iteration [474], Training Loss: [0.1387]\n",
      "Epoch [1], Iteration [475], Training Loss: [0.0421]\n",
      "Epoch [1], Iteration [476], Training Loss: [0.1126]\n",
      "Epoch [1], Iteration [477], Training Loss: [0.0055]\n",
      "Epoch [1], Iteration [478], Training Loss: [0.0110]\n",
      "Epoch [1], Iteration [479], Training Loss: [0.0031]\n",
      "Epoch [1], Iteration [480], Training Loss: [0.0028]\n",
      "Epoch [1], Iteration [481], Training Loss: [0.0174]\n",
      "Epoch [1], Iteration [482], Training Loss: [0.0631]\n",
      "Epoch [1], Iteration [483], Training Loss: [0.0151]\n",
      "Epoch [1], Iteration [484], Training Loss: [0.2308]\n",
      "Epoch [1], Iteration [485], Training Loss: [0.0154]\n",
      "Epoch [1], Iteration [486], Training Loss: [0.1156]\n",
      "Epoch [1], Iteration [487], Training Loss: [0.1582]\n",
      "Epoch [1], Iteration [488], Training Loss: [0.0026]\n",
      "Epoch [1], Iteration [489], Training Loss: [0.1076]\n",
      "Epoch [1], Iteration [490], Training Loss: [0.0417]\n",
      "Epoch [1], Iteration [491], Training Loss: [0.0024]\n",
      "Epoch [1], Iteration [492], Training Loss: [0.0097]\n",
      "Epoch [1], Iteration [493], Training Loss: [0.0068]\n",
      "Epoch [1], Iteration [494], Training Loss: [0.0917]\n",
      "Epoch [1], Iteration [495], Training Loss: [0.1075]\n",
      "Epoch [1], Iteration [496], Training Loss: [0.0870]\n",
      "Epoch [1], Iteration [497], Training Loss: [0.2006]\n",
      "Epoch [1], Iteration [498], Training Loss: [0.0422]\n",
      "Epoch [1], Iteration [499], Training Loss: [0.1036]\n",
      "Epoch [1], Iteration [500], Training Loss: [0.0131]\n",
      "Epoch [1], Iteration [501], Training Loss: [0.0008]\n",
      "Epoch [1], Iteration [502], Training Loss: [0.0019]\n",
      "Epoch [1], Iteration [503], Training Loss: [0.0235]\n",
      "Epoch [1], Iteration [504], Training Loss: [0.0119]\n",
      "Epoch [1], Iteration [505], Training Loss: [0.0350]\n",
      "Epoch [1], Iteration [506], Training Loss: [0.0589]\n",
      "Epoch [1], Iteration [507], Training Loss: [0.1791]\n",
      "Epoch [1], Iteration [508], Training Loss: [0.0204]\n",
      "Epoch [1], Iteration [509], Training Loss: [0.3403]\n",
      "Epoch [1], Iteration [510], Training Loss: [0.2168]\n",
      "Epoch [1], Iteration [511], Training Loss: [0.0842]\n",
      "Epoch [1], Iteration [512], Training Loss: [0.0054]\n",
      "Epoch [1], Iteration [513], Training Loss: [0.1894]\n",
      "Epoch [1], Iteration [514], Training Loss: [0.1162]\n",
      "Epoch [1], Iteration [515], Training Loss: [0.0078]\n",
      "Epoch [1], Iteration [516], Training Loss: [0.1121]\n",
      "Epoch [1], Iteration [517], Training Loss: [0.0828]\n",
      "Epoch [1], Iteration [518], Training Loss: [0.0508]\n",
      "Epoch [1], Iteration [519], Training Loss: [0.0806]\n",
      "Epoch [1], Iteration [520], Training Loss: [0.0193]\n",
      "Epoch [1], Iteration [521], Training Loss: [0.0210]\n",
      "Epoch [1], Iteration [522], Training Loss: [0.0416]\n",
      "Epoch [1], Iteration [523], Training Loss: [0.0642]\n",
      "Epoch [1], Iteration [524], Training Loss: [0.0078]\n",
      "Epoch [1], Iteration [525], Training Loss: [0.0109]\n",
      "Epoch [1], Iteration [526], Training Loss: [0.0464]\n",
      "Epoch [1], Iteration [527], Training Loss: [0.0567]\n",
      "Epoch [1], Iteration [528], Training Loss: [0.0320]\n",
      "Epoch [1], Iteration [529], Training Loss: [0.0149]\n",
      "Epoch [1], Iteration [530], Training Loss: [0.0089]\n",
      "Epoch [1], Iteration [531], Training Loss: [0.0227]\n",
      "Epoch [1], Iteration [532], Training Loss: [0.1215]\n",
      "Epoch [1], Iteration [533], Training Loss: [0.0403]\n",
      "Epoch [1], Iteration [534], Training Loss: [0.0483]\n",
      "Epoch [1], Iteration [535], Training Loss: [0.0235]\n",
      "Epoch [1], Iteration [536], Training Loss: [0.1322]\n",
      "Epoch [1], Iteration [537], Training Loss: [0.0022]\n",
      "Epoch [1], Iteration [538], Training Loss: [0.0636]\n",
      "Epoch [1], Iteration [539], Training Loss: [0.0476]\n",
      "Epoch [1], Iteration [540], Training Loss: [0.0507]\n",
      "Epoch [1], Iteration [541], Training Loss: [0.0630]\n",
      "Epoch [1], Iteration [542], Training Loss: [0.0540]\n",
      "Epoch [1], Iteration [543], Training Loss: [0.1033]\n",
      "Epoch [1], Iteration [544], Training Loss: [0.0169]\n",
      "Epoch [1], Iteration [545], Training Loss: [0.0536]\n",
      "Epoch [1], Iteration [546], Training Loss: [0.0796]\n",
      "Epoch [1], Iteration [547], Training Loss: [0.0869]\n",
      "Epoch [1], Iteration [548], Training Loss: [0.0487]\n",
      "Epoch [1], Iteration [549], Training Loss: [0.0608]\n",
      "Epoch [1], Iteration [550], Training Loss: [0.0286]\n",
      "Epoch [1], Iteration [551], Training Loss: [0.0042]\n",
      "Epoch [1], Iteration [552], Training Loss: [0.0077]\n",
      "Epoch [1], Iteration [553], Training Loss: [0.0646]\n",
      "Epoch [1], Iteration [554], Training Loss: [0.0970]\n",
      "Epoch [1], Iteration [555], Training Loss: [0.0641]\n",
      "Epoch [1], Iteration [556], Training Loss: [0.0101]\n",
      "Epoch [1], Iteration [557], Training Loss: [0.1510]\n",
      "Epoch [1], Iteration [558], Training Loss: [0.0045]\n",
      "Epoch [1], Iteration [559], Training Loss: [0.0118]\n",
      "Epoch [1], Iteration [560], Training Loss: [0.0177]\n",
      "Epoch [1], Iteration [561], Training Loss: [0.0105]\n",
      "Epoch [1], Iteration [562], Training Loss: [0.0174]\n",
      "Epoch [1], Iteration [563], Training Loss: [0.0157]\n",
      "Epoch [1], Iteration [564], Training Loss: [0.1118]\n",
      "Epoch [1], Iteration [565], Training Loss: [0.0105]\n",
      "Epoch [1], Iteration [566], Training Loss: [0.0045]\n",
      "Epoch [1], Iteration [567], Training Loss: [0.0086]\n",
      "Epoch [1], Iteration [568], Training Loss: [0.0052]\n",
      "Epoch [1], Iteration [569], Training Loss: [0.0210]\n",
      "Epoch [1], Iteration [570], Training Loss: [0.0113]\n",
      "Epoch [1], Iteration [571], Training Loss: [0.1581]\n",
      "Epoch [1], Iteration [572], Training Loss: [0.0361]\n",
      "Epoch [1], Iteration [573], Training Loss: [0.0074]\n",
      "Epoch [1], Iteration [574], Training Loss: [0.1135]\n",
      "Epoch [1], Iteration [575], Training Loss: [0.0088]\n",
      "Epoch [1], Iteration [576], Training Loss: [0.0215]\n",
      "Epoch [1], Iteration [577], Training Loss: [0.0676]\n",
      "Epoch [1], Iteration [578], Training Loss: [0.0116]\n",
      "Epoch [1], Iteration [579], Training Loss: [0.0881]\n",
      "Epoch [1], Iteration [580], Training Loss: [0.0358]\n",
      "Epoch [1], Iteration [581], Training Loss: [0.0132]\n",
      "Epoch [1], Iteration [582], Training Loss: [0.0865]\n",
      "Epoch [1], Iteration [583], Training Loss: [0.1208]\n",
      "Epoch [1], Iteration [584], Training Loss: [0.0202]\n",
      "Epoch [1], Iteration [585], Training Loss: [0.0328]\n",
      "Epoch [1], Iteration [586], Training Loss: [0.1130]\n",
      "Epoch [1], Iteration [587], Training Loss: [0.0104]\n",
      "Epoch [1], Iteration [588], Training Loss: [0.0167]\n",
      "Epoch [1], Iteration [589], Training Loss: [0.0093]\n",
      "Epoch [1], Iteration [590], Training Loss: [0.0851]\n",
      "Epoch [1], Iteration [591], Training Loss: [0.0067]\n",
      "Epoch [1], Iteration [592], Training Loss: [0.0239]\n",
      "Epoch [1], Iteration [593], Training Loss: [0.0054]\n",
      "Epoch [1], Iteration [594], Training Loss: [0.0022]\n",
      "Epoch [1], Iteration [595], Training Loss: [0.0499]\n",
      "Epoch [1], Iteration [596], Training Loss: [0.0832]\n",
      "Epoch [1], Iteration [597], Training Loss: [0.0156]\n",
      "Epoch [1], Iteration [598], Training Loss: [0.0280]\n",
      "Epoch [1], Iteration [599], Training Loss: [0.0143]\n",
      "Epoch [1], Iteration [600], Training Loss: [0.0437]\n",
      "Epoch [1], Iteration [601], Training Loss: [0.0450]\n",
      "Epoch [1], Iteration [602], Training Loss: [0.0062]\n",
      "Epoch [1], Iteration [603], Training Loss: [0.0133]\n",
      "Epoch [1], Iteration [604], Training Loss: [0.0154]\n",
      "Epoch [1], Iteration [605], Training Loss: [0.0255]\n",
      "Epoch [1], Iteration [606], Training Loss: [0.0248]\n",
      "Epoch [1], Iteration [607], Training Loss: [0.0021]\n",
      "Epoch [1], Iteration [608], Training Loss: [0.2146]\n",
      "Epoch [1], Iteration [609], Training Loss: [0.0309]\n",
      "Epoch [1], Iteration [610], Training Loss: [0.1029]\n",
      "Epoch [1], Iteration [611], Training Loss: [0.0211]\n",
      "Epoch [1], Iteration [612], Training Loss: [0.0968]\n",
      "Epoch [1], Iteration [613], Training Loss: [0.0172]\n",
      "Epoch [1], Iteration [614], Training Loss: [0.2386]\n",
      "Epoch [1], Iteration [615], Training Loss: [0.0146]\n",
      "Epoch [1], Iteration [616], Training Loss: [0.1324]\n",
      "Epoch [1], Iteration [617], Training Loss: [0.0960]\n",
      "Epoch [1], Iteration [618], Training Loss: [0.1050]\n",
      "Epoch [1], Iteration [619], Training Loss: [0.0060]\n",
      "Epoch [1], Iteration [620], Training Loss: [0.0457]\n",
      "Epoch [1], Iteration [621], Training Loss: [0.0394]\n",
      "Epoch [1], Iteration [622], Training Loss: [0.0305]\n",
      "Epoch [1], Iteration [623], Training Loss: [0.2141]\n",
      "Epoch [1], Iteration [624], Training Loss: [0.0193]\n",
      "Epoch [1], Iteration [625], Training Loss: [0.1543]\n",
      "Epoch [1], Iteration [626], Training Loss: [0.0628]\n",
      "Epoch [1], Iteration [627], Training Loss: [0.1852]\n",
      "Epoch [1], Iteration [628], Training Loss: [0.0211]\n",
      "Epoch [1], Iteration [629], Training Loss: [0.0268]\n",
      "Epoch [1], Iteration [630], Training Loss: [0.0358]\n",
      "Epoch [1], Iteration [631], Training Loss: [0.0094]\n",
      "Epoch [1], Iteration [632], Training Loss: [0.0128]\n",
      "Epoch [1], Iteration [633], Training Loss: [0.0261]\n",
      "Epoch [1], Iteration [634], Training Loss: [0.0088]\n",
      "Epoch [1], Iteration [635], Training Loss: [0.0693]\n",
      "Epoch [1], Iteration [636], Training Loss: [0.0052]\n",
      "Epoch [1], Iteration [637], Training Loss: [0.0417]\n",
      "Epoch [1], Iteration [638], Training Loss: [0.0074]\n",
      "Epoch [1], Iteration [639], Training Loss: [0.0136]\n",
      "Epoch [1], Iteration [640], Training Loss: [0.0177]\n",
      "Epoch [1], Iteration [641], Training Loss: [0.0021]\n",
      "Epoch [1], Iteration [642], Training Loss: [0.1229]\n",
      "Epoch [1], Iteration [643], Training Loss: [0.0109]\n",
      "Epoch [1], Iteration [644], Training Loss: [0.0020]\n",
      "Epoch [1], Iteration [645], Training Loss: [0.0284]\n",
      "Epoch [1], Iteration [646], Training Loss: [0.0127]\n",
      "Epoch [1], Iteration [647], Training Loss: [0.0091]\n",
      "Epoch [1], Iteration [648], Training Loss: [0.0035]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\Resources\\BASE\\Simple model.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m opt \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(basemodel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m##########################################################\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m# Training the model until the full real dataset is used #\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m##########################################################\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (X_train_gen, y_train_gen) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(gen_train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         \u001b[39m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         basemodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         \u001b[39m# Move the data to the device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\Resources\\BASE\\Simple model.ipynb Cell 28\u001b[0m in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m img_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages[idx]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, img_name)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(img_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m image \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wassi/Documents/GitHub/RecogniChess/Resources/BASE/Simple%20model.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[\u001b[39mint\u001b[39m(img_name[\u001b[39m3\u001b[39m:\u001b[39m9\u001b[39m])]\n",
      "File \u001b[1;32mc:\\Users\\wassi\\Documents\\GitHub\\RecogniChess\\myenv\\lib\\site-packages\\PIL\\Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2950\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 2953\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   2954\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2956\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# To store the best model\n",
    "best_model = None\n",
    "best_balanced_accuracy = 0\n",
    "\n",
    "for gamma in gamma_focal_loss_choices:\n",
    "\n",
    "    for dropout_rate in dropout_rate_choices:\n",
    "        \n",
    "        # Define the new loss function\n",
    "        # focal_loss = torch.hub.load(\n",
    "        #     'adeelh/pytorch-multi-class-focal-loss',\n",
    "        #     model='FocalLoss',\n",
    "        #     alpha=torch.tensor(class_proportions),\n",
    "        #     gamma=gamma,\n",
    "        #     reduction='mean',\n",
    "        #     force_reload=False,\n",
    "        #     verbose = False\n",
    "        # ).to(DEVICE)\n",
    "        focal_loss = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "        # To store the metrics through epochs\n",
    "        training_loss_through_iterations = np.array[])\n",
    "        gen_validation_loss_through_epochs = np.array([])\n",
    "        gen_validation_acc_through_epochs = np.array([])\n",
    "        gen_validation_f1_through_epochs = np.array([])\n",
    "        real_validation_loss_through_epochs = np.array([])\n",
    "        real_validation_acc_through_epochs = np.array([])\n",
    "        real_validation_f1_through_epochs = np.array([])\n",
    "\n",
    "        basemodel = BaseModel(dropout_rate=dropout_rate).to(DEVICE)\n",
    "        opt = optim.Adam(basemodel.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            ##########################################################\n",
    "            # Training the model until the full real dataset is used #\n",
    "            ##########################################################\n",
    "            for i, (X_train_gen, y_train_gen) in enumerate(gen_train_loader):\n",
    "                \n",
    "                # Set the model to training mode\n",
    "                basemodel.train()\n",
    "\n",
    "                # Move the data to the device\n",
    "                X_train_gen = X_train_gen.to(DEVICE)\n",
    "                y_train_gen = y_train_gen.to(DEVICE)\n",
    "\n",
    "                # Forward pass\n",
    "                y_train_pred_prob_gen = basemodel(X_train_gen)\n",
    "                y_train_pred_gen = torch.argmax(y_train_pred_prob_gen, dim=1)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = focal_loss(y_train_pred_prob_gen, y_train_gen.long())\n",
    "\n",
    "                # Backward pass\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                # Print the loss & save it every n_loss iteration\n",
    "                print(f'Epoch [{epoch + 1}], Iteration [{i + 1}], Training Loss: [{loss.item():.4f}]')\n",
    "                training_loss_through_iterations = np.append(training_loss_through_iterations, (loss.item(), i+1))\n",
    "           \n",
    "\n",
    "            ########################################################\n",
    "            # Evaluate the model every epoch on the validation set #\n",
    "            ########################################################\n",
    "\n",
    "            # Set the model to evaluation mode\n",
    "            basemodel.eval()\n",
    "            \n",
    "            # Disable gradient calculation\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # 1) Evaluate on the generated validation set\n",
    "                # TODO: balanced_acc_sum = 0\n",
    "                weighted_f1_sum = 0\n",
    "\n",
    "                # Extract the iterator from the data loader\n",
    "                gen_val_iter = iter(gen_val_loader)\n",
    "\n",
    "                # Iterate for n_eval_minibatches\n",
    "                for i in range(n_eval_minibatches):\n",
    "\n",
    "                    # Get the next minibatch\n",
    "                    X_val_gen, y_val_gen = next(gen_val_iter)\n",
    "                        \n",
    "                    # Move the data to the device\n",
    "                    X_val_gen = X_val_gen.to(DEVICE)\n",
    "                    y_val_gen = y_val_gen.to(DEVICE)\n",
    "\n",
    "                    # Forward pass\n",
    "                    y_val_pred_prob_gen = basemodel(X_val_gen)\n",
    "                    y_val_pred_gen = torch.argmax(y_train_pred_prob_gen, dim=1)\n",
    "\n",
    "                    # Compute the balanced accuracy score\n",
    "                    # TODO: balanced_acc_sum += metrics.balanced_accuracy_score(y_val_gen.long().cpu(), y_val_pred_gen.long().cpu(), sample_weight=None)\n",
    "                    weighted_f1_sum += f1_score(y_val_pred_gen, y_val_gen)\n",
    "\n",
    "                # Compute the average balanced accuracy score & loss\n",
    "                # TODO: balanced_acc_gen = balanced_acc_sum / n_eval_minibatches\n",
    "                loss_gen = focal_loss(y_val_pred_prob_gen, y_val_gen)\n",
    "                weighted_f1_gen = weighted_f1_sum / n_eval_minibatches\n",
    "\n",
    "                # # 2) Evaluate on the real validation set\n",
    "                # # TODO: balanced_acc_sum = 0\n",
    "                # weighted_f1_sum = 0\n",
    "\n",
    "                # # Iterate for n_eval_minibatches\n",
    "                # for (X_val_real, y_val_real) in real_val_loader:\n",
    "\n",
    "                #     # Move the data to the device\n",
    "                #     X_val_real = X_val_real.to(DEVICE)\n",
    "                #     y_val_real = y_val_real.to(DEVICE)\n",
    "\n",
    "                #     # Forward pass\n",
    "                #     y_val_pred_prob_real = basemodel(X_val_real)\n",
    "                #     y_val_pred_real = torch.argmax(y_val_pred_prob_real, dim=1)\n",
    "\n",
    "                #     # Compute the balanced accuracy score\n",
    "                #     # TODO: balanced_acc_sum += metrics.balanced_accuracy_score(y_val_real.long().cpu(), y_val_pred_real.long().cpu(), sample_weight=None) # TODO: Add sample weights for all 13 classes\n",
    "                #     weighted_f1_sum += f1_score(y_val_pred_real, y_val_real)\n",
    "\n",
    "                # # Compute the average balanced accuracy score\n",
    "                # # TODO: balanced_acc_real = balanced_acc_sum / n_eval_minibatches\n",
    "                # loss_real = focal_loss(y_val_pred_prob_real, y_val_real)\n",
    "                # weighted_f1_real = weighted_f1_sum / n_eval_minibatches\n",
    "\n",
    "                # Store all 6 metrics\n",
    "                # TODO: gen_validation_acc_through_epochs = np.append(gen_validation_acc_through_epochs, (balanced_acc_gen, epoch+1))\n",
    "                # TODO: real_validation_acc_through_epochs = np.append(real_validation_acc_through_epochs, (balanced_acc_real, epoch+1))\n",
    "                gen_validation_loss_through_epochs = np.append(gen_validation_loss_through_epochs, (loss_gen.item(), epoch+1))\n",
    "                # real_validation_loss_through_epochs = np.append(real_validation_loss_through_epochs, (loss_real.item(), epoch+1))\n",
    "                gen_validation_f1_through_epochs = np.append(gen_validation_f1_through_epochs, (weighted_f1_gen.item(), epoch+1))\n",
    "                # real_validation_f1_through_epochs = np.append(real_validation_f1_through_epochs, (weighted_f1_real.item(), epoch+1))\n",
    "\n",
    "                # TODO: Remove this\n",
    "                print(f'VALIDATION => Epoch [{epoch + 1}], Balanced Accuracy (Gen): [TODO], Balanced Accuracy (Real): [TODO]')\n",
    "                print(f'VALIDATION => Epoch [{epoch + 1}], F1 Score (Gen): [{weighted_f1_gen:.4f}], F1 Score (Real): [TODO]')\n",
    "\n",
    "\n",
    "            ##############################################\n",
    "            # Save the model every epoch as a checkpoint #\n",
    "            ##############################################\n",
    "\n",
    "            torch.save(basemodel.state_dict(), f'./checkpoints/basemodel_{epoch+1}.ckpt')\n",
    "\n",
    "\n",
    "        # Compare to the best model and save this one if it is better\n",
    "        if best_model is None or best_balanced_accuracy < balanced_acc_real:\n",
    "            best_model = basemodel\n",
    "            best_balanced_accuracy = balanced_acc_real\n",
    "            torch.save(basemodel.state_dict(), f'./best_model_gamma_{gamma}_dropout_{dropout_rate}.ckpt')\n",
    "\n",
    "\n",
    "        # Save the training & validation metrics\n",
    "        np.save(f'./training_loss_gamma_{gamma}_dropout_{dropout_rate}.npy', training_loss_through_iterations, allow_pickle=True)\n",
    "        np.save(f'./gen_validation_loss_gamma_{gamma}_dropout_{dropout_rate}.npy', gen_validation_loss_through_epochs, allow_pickle=True)\n",
    "        np.save(f'./gen_validation_acc_gamma_{gamma}_dropout_{dropout_rate}.npy', gen_validation_acc_through_epochs, allow_pickle=True)\n",
    "        np.save(f'./real_validation_loss_gamma_{gamma}_dropout_{dropout_rate}.npy', real_validation_loss_through_epochs, allow_pickle=True)\n",
    "        np.save(f'./real_validation_acc_gamma_{gamma}_dropout_{dropout_rate}.npy', real_validation_acc_through_epochs, allow_pickle=True)\n",
    "        np.save(f'./gen_validation_f1_gamma_{gamma}_dropout_{dropout_rate}.npy', gen_validation_f1_through_epochs, allow_pickle=True)\n",
    "        np.save(f'./real_validation_f1_gamma_{gamma}_dropout_{dropout_rate}.npy', real_validation_f1_through_epochs, allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64e5fcf3",
   "metadata": {},
   "source": [
    "Now that the best model has been found, we can use it to compute the testing accuracy & f1 score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
